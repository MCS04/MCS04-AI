{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUANWN3rpfC9"
      },
      "source": [
        "# 0. Setup Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQGS5KceQaQE",
        "outputId": "5725bdf6-36d3-492d-dc77-eeb977a2f22d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Tensorflow': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# remove Tensorflow directory to reset\n",
        "!rm -r Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "146BB11JpfDA"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "42hJEdo_pfDB"
      },
      "outputs": [],
      "source": [
        "# define constants to use\n",
        "CUSTOM_MODEL_NAME = 'my_ssd_mobnet'\n",
        "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\n",
        "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz'\n",
        "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
        "LABEL_MAP_NAME = 'label_map.pbtxt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hbPhYVy_pfDB"
      },
      "outputs": [],
      "source": [
        "# define all the paths to use within the workspace\n",
        "paths = {\n",
        "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
        "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
        "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
        "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
        "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
        "    'IMAGE_TRAIN_PATH': os.path.join('Tensorflow', 'workspace','images', 'train'),\n",
        "    'IMAGE_VALIDATE_PATH': os.path.join('Tensorflow', 'workspace','images', 'valid'),\n",
        "    'IMAGE_TEST_PATH': os.path.join('Tensorflow', 'workspace','images', 'test'),\n",
        "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
        "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
        "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME),\n",
        "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'),\n",
        "    'INFERENCE_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'inference'),\n",
        "    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'),\n",
        "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'),\n",
        "    'PROTOC_PATH':os.path.join('Tensorflow','protoc'),\n",
        "    'ROBOFLOW_TRAIN_PATH':os.path.join('Bird-Nest-15', 'train'),\n",
        "    'ROBOFLOW_VALIDATE_PATH':os.path.join('Bird-Nest-15', 'valid'),\n",
        "    'ROBOFLOW_TEST_PATH':os.path.join('Bird-Nest-15', 'test')\n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LwhWZMI0pfDC"
      },
      "outputs": [],
      "source": [
        "files = {\n",
        "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
        "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME),\n",
        "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HR-TfDGrpfDC"
      },
      "outputs": [],
      "source": [
        "# create the directories for all the paths defined\n",
        "for path in paths.values():\n",
        "    if not os.path.exists(path):\n",
        "        if os.name == 'posix':\n",
        "            !mkdir -p {path}\n",
        "        if os.name == 'nt':\n",
        "            !mkdir {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLU-rs_ipfDE"
      },
      "source": [
        "# 1. Download TF Models Pretrained Models from Tensorflow Model Zoo and Install TFOD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "u-8UWGw0ajWU"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/install/source_windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K-Cmz2edpfDE",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# windows OS install wget\n",
        "if os.name=='nt':\n",
        "    !pip install wget\n",
        "    import wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA1DIq5OpfDE",
        "outputId": "66377668-9d06-493d-9169-18c5fe870eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Tensorflow/models'...\n",
            "remote: Enumerating objects: 88134, done.\u001b[K\n",
            "remote: Counting objects: 100% (1846/1846), done.\u001b[K\n",
            "remote: Compressing objects: 100% (765/765), done.\u001b[K\n",
            "remote: Total 88134 (delta 1211), reused 1643 (delta 1063), pack-reused 86288\u001b[K\n",
            "Receiving objects: 100% (88134/88134), 603.02 MiB | 34.10 MiB/s, done.\n",
            "Resolving deltas: 100% (63109/63109), done.\n"
          ]
        }
      ],
      "source": [
        "# clone the Tensorflow object detection api repo\n",
        "if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection')):\n",
        "    !git clone https://github.com/tensorflow/models {paths['APIMODEL_PATH']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJjMHbnDs3Tv",
        "outputId": "7659ec68-44b8-4679-fd36-c1dc1f5c9f8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.12.4-1ubuntu7.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "Processing /content/Tensorflow/models/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting avro-python3 (from object-detection==0.1)\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting apache-beam (from object-detection==0.1)\n",
            "  Downloading apache_beam-2.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (9.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (4.9.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.7.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (21.6.0)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.0.7)\n",
            "Collecting lvis (from object-detection==0.1)\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.11.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.5.3)\n",
            "Collecting tf-models-official>=2.5.1 (from object-detection==0.1)\n",
            "  Downloading tf_models_official-2.14.2-py2.py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow_io (from object-detection==0.1)\n",
            "  Downloading tensorflow_io-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.14.0)\n",
            "Collecting pyparsing==2.4.7 (from object-detection==0.1)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu<=2.2.0 (from object-detection==0.1)\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.23.5)\n",
            "Collecting colorama (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.84.0)\n",
            "Collecting immutabledict (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading immutabledict-3.0.0-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.16)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.8.1.78)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (6.0.1)\n",
            "Collecting sentencepiece (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.9.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.15.0)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text~=2.14.0 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2023.3.post1)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim->object-detection==0.1) (1.4.0)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam->object-detection==0.1)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam->object-detection==0.1)\n",
            "  Downloading fastavro-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam->object-detection==0.1)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.59.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.22.0)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam->object-detection==0.1)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting objsize<0.7.0,>=0.6.1 (from apache-beam->object-detection==0.1)\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (23.2)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.0,!=4.24.1,!=4.24.2,<4.25.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.5.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (1.4.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (4.8.0.76)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (1.1.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (4.43.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.34.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io->object-detection==0.1) (0.34.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam->object-detection==0.1) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam->object-detection==0.1)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2023.7.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (2.14.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.41.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (6.1.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.61.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (2.1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n",
            "Building wheels for collected packages: object-detection, avro-python3, crcmod, dill, hdfs, seqeval, pyjsparser, docopt\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1697354 sha256=2129885d1a7febd2dd3aafd1057f63d19d7805462077b3f6a3bef912b2af92d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ld__h9ny/wheels/fb/c9/43/709f88e66b36649c7a29812ca4f6236f31caed949aabc3e335\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43991 sha256=89d61657f1004e1c2bc2fd77598e39ee6305154781385d1d261241ff859e106d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31406 sha256=9552de3728f426065481ca2b9b01ec469f4b251a215f4493782d7b5941d252ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=289e9f5cd9e464850cf0d69290b0e87b60ca9d98c5912034e7581215cb6507c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=f595aba9315b96f3eeff40e51ca1144ba14f572f622eb5ea25a137e01a78184e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=a279002d07a1b25272120e3f699b100de456b0c808b11d84b2b92211271da20e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25982 sha256=93a2f8d56db22d5a0898cdbf89d74b39d8413d2273cf86f825bcaa250c0e9981\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=41b7cfa6663d7179a7884aa577e316f4a5a7bc13bf0b128f232639ece20603e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built object-detection avro-python3 crcmod dill hdfs seqeval pyjsparser docopt\n",
            "Installing collected packages: sentencepiece, pyjsparser, docopt, crcmod, zstandard, tensorflow-model-optimization, tensorflow_io, pyparsing, portalocker, orjson, objsize, js2py, immutabledict, fasteners, fastavro, dnspython, dill, colorama, avro-python3, sacrebleu, pymongo, hdfs, seqeval, lvis, apache-beam, tensorflow-text, tf-models-official, object-detection\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "Successfully installed apache-beam-2.51.0 avro-python3-1.10.2 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.4.2 docopt-0.6.2 fastavro-1.9.0 fasteners-0.19 hdfs-2.7.3 immutabledict-3.0.0 js2py-0.74 lvis-0.5.3 object-detection-0.1 objsize-0.6.1 orjson-3.9.10 portalocker-2.8.2 pyjsparser-2.7.1 pymongo-4.5.0 pyparsing-2.4.7 sacrebleu-2.2.0 sentencepiece-0.1.99 seqeval-1.2.2 tensorflow-model-optimization-0.7.5 tensorflow-text-2.14.0 tensorflow_io-0.34.0 tf-models-official-2.14.2 zstandard-0.21.0\n"
          ]
        }
      ],
      "source": [
        "# Install Tensorflow Object Detection\n",
        "if os.name=='posix':\n",
        "    !apt-get install protobuf-compiler\n",
        "    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install .\n",
        "\n",
        "if os.name=='nt':\n",
        "    url=\"https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip\"\n",
        "    wget.download(url)\n",
        "    !move protoc-3.15.6-win64.zip {paths['PROTOC_PATH']}\n",
        "    !cd {paths['PROTOC_PATH']} && tar -xf protoc-3.15.6-win64.zip\n",
        "    os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['PROTOC_PATH'], 'bin'))\n",
        "    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && copy object_detection\\\\packages\\\\tf2\\\\setup.py setup.py && python setup.py build && python setup.py install\n",
        "    !cd Tensorflow/models/research/slim && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcc5MerWajWW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# run the verification script\n",
        "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
        "# Verify Installation\n",
        "!python {VERIFICATION_SCRIPT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BhyyztLLajWW",
        "outputId": "8ea0a953-7d78-4c47-e0d4-1824da29d372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.13.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.5.26)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.14.0 requires tensorflow<2.15,>=2.14.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.13.0 which is incompatible.\n",
            "tf-models-official 2.14.2 requires tensorflow~=2.14.0, but you have tensorflow 2.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0\n"
          ]
        }
      ],
      "source": [
        "# for macOS users\n",
        "!pip install tensorflow==2.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I0hrkPC1ajWW"
      },
      "outputs": [],
      "source": [
        "!pip uninstall protobuf matplotlib -y\n",
        "!pip install protobuf==3.20 matplotlib==3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CGpqQ5sRajWW"
      },
      "outputs": [],
      "source": [
        "import object_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOu5SZXNajWW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csofht2npfDE",
        "outputId": "734984da-3b21-4148-85c6-7c2650a1ca47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-28 08:52:02--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.161.207, 74.125.126.207, 74.125.132.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.161.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20518283 (20M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz’\n",
            "\n",
            "ssd_mobilenet_v2_fp 100%[===================>]  19.57M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-10-28 08:52:02 (181 MB/s) - ‘ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz’ saved [20518283/20518283]\n",
            "\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/checkpoint\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0.index\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/pipeline.config\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/saved_model.pb\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/variables/\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
            "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model/variables/variables.index\n"
          ]
        }
      ],
      "source": [
        "# move pretrained model to workspace path\n",
        "if os.name =='posix':\n",
        "    !wget {PRETRAINED_MODEL_URL}\n",
        "    !mv {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
        "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}\n",
        "if os.name == 'nt':\n",
        "    wget.download(PRETRAINED_MODEL_URL)\n",
        "    !move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
        "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5KJTnkfpfDC"
      },
      "source": [
        "# 2. Create Label Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p1BVDWo7pfDC"
      },
      "outputs": [],
      "source": [
        "# defining all the labels\n",
        "labels = [{'name':'0', 'id':1, 'display_name': '0'}, {'name':'1', 'id':2, 'display_name': '1'}, {'name':'2', 'id':3, 'display_name': '2'}]\n",
        "\n",
        "# open and write out a label map file\n",
        "with open(files['LABELMAP'], 'w') as f:\n",
        "    for label in labels:\n",
        "        f.write('item { \\n')\n",
        "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
        "        f.write('\\tid:{}\\n'.format(label['id']))\n",
        "        f.write('\\tdisplay_name:\\'{}\\'\\n'.format(label['display_name']))\n",
        "        f.write('}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C88zyVELpfDC"
      },
      "source": [
        "# 3. Download images and TF records from Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sCw4gssyPvGg"
      },
      "outputs": [],
      "source": [
        "# remove the bird nest image directory\n",
        "!rm -r Bird-Nest-15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZSRueT1djHp3",
        "outputId": "ca384cc6-6118-4d81-c2f7-f55a8f4d2801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.7-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi==2022.12.7 (from roboflow)\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting idna==2.10 (from roboflow)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Collecting supervision (from roboflow)\n",
            "  Downloading supervision-0.16.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.1.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.43.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.3)\n",
            "Installing collected packages: python-dotenv, opencv-python-headless, idna, cycler, chardet, certifi, supervision, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.8.1.78\n",
            "    Uninstalling opencv-python-headless-4.8.1.78:\n",
            "      Successfully uninstalled opencv-python-headless-4.8.1.78\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.7.22\n",
            "    Uninstalling certifi-2023.7.22:\n",
            "      Successfully uninstalled certifi-2023.7.22\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow-text 2.14.0 requires tensorflow<2.15,>=2.14.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.13.0 which is incompatible.\n",
            "tf-models-official 2.14.2 requires tensorflow~=2.14.0, but you have tensorflow 2.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2022.12.7 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 python-dotenv-1.0.0 requests-toolbelt-1.0.0 roboflow-1.1.7 supervision-0.16.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Bird-Nest-15 to tensorflow:: 100%|██████████| 87677/87677 [00:03<00:00, 28525.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Bird-Nest-15 in tensorflow:: 100%|██████████| 775/775 [00:00<00:00, 2424.58it/s]\n"
          ]
        }
      ],
      "source": [
        "# download the dataset of ebn images from roboflow\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"1woWfE1q4RoyHytXmktz\")\n",
        "project = rf.workspace(\"ebn\").project(\"bird-nest-exr6l\")\n",
        "dataset = project.version(15).download(\"tensorflow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kEkIJhiN0Qwo"
      },
      "outputs": [],
      "source": [
        "# move the annotations CSV files to the annotation path\n",
        "import glob\n",
        "\n",
        "source_train = os.path.join(paths['ROBOFLOW_TRAIN_PATH'],'_annotations.csv')\n",
        "destination_train = os.path.join(paths['ANNOTATION_PATH'], 'train.csv')\n",
        "\n",
        "source_test = os.path.join(paths['ROBOFLOW_TEST_PATH'],'_annotations.csv')\n",
        "destination_test = os.path.join(paths['ANNOTATION_PATH'], 'test.csv')\n",
        "\n",
        "source_valid = os.path.join(paths['ROBOFLOW_VALIDATE_PATH'],'_annotations.csv')\n",
        "destination_valid = os.path.join(paths['ANNOTATION_PATH'], 'valid.csv')\n",
        "\n",
        "os.rename(source_train, destination_train)\n",
        "os.rename(source_test, destination_test)\n",
        "os.rename(source_valid, destination_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjGE1b9RoeuP",
        "outputId": "4a82044c-d1d2-49f7-b7c1-6211b76eaedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moved: BrokenBig-3-_bmp.rf.c6f1d10047d887599e95078db2dcb4d3.jpg\n",
            "Moved: SizeB-15-_bmp.rf.f928a9bfb340a2131702fd913b5babac.jpg\n",
            "Moved: WhiteBeige-8-_bmp.rf.089dc74a553da57ee83a3020d2d5a71e.jpg\n",
            "Moved: HeavyFeather-9-_bmp.rf.9139d52f57c3bcca284c4bb6c2943a4a.jpg\n",
            "Moved: SizeB-8-_bmp.rf.d04481ae2b66897273c23e66fea17560.jpg\n",
            "Moved: MedFeather-9-_bmp.rf.90e9f4bfcd0b6c4a0897c598bc6492b8.jpg\n",
            "Moved: Yellow-19-_bmp.rf.87681c0976b7b05650fc2a83416f7e1d.jpg\n",
            "Moved: Beige-11-_bmp.rf.d9de731a00c7e286cda3c70f5c0bc9e1.jpg\n",
            "Moved: MedFeather-15-_bmp.rf.206629139c28657340e513a70a919b5e.jpg\n",
            "Moved: Triangular-6-_bmp.rf.b0d23d9e2128bcd60fa2a3de37300516.jpg\n",
            "Moved: White-7-_bmp.rf.9b65cac2f8d1c934766d1b4920023143.jpg\n",
            "Moved: SizeA-4-_bmp.rf.6e0312a7d7a1aa6201ca61f977e6ff6b.jpg\n",
            "Moved: WhiteBeige-13-_bmp.rf.89e591cdc0f9a0471f0f2c662a090fbe.jpg\n",
            "Moved: LightFeather-8-_bmp.rf.a379d0b07f40ad4f127476f3d01583bc.jpg\n",
            "Moved: BrokenSmall-34-_bmp.rf.07ad6ae9406d57150a81ad2347082c27.jpg\n",
            "Moved: SizeA-13-_bmp.rf.6fa66596a4d7f6f65620ea5238aef24d.jpg\n",
            "Moved: Ping-11-_bmp.rf.0af3add3415bc7a3426b40b3f759a763.jpg\n",
            "Moved: Strips-10-_bmp.rf.c9c7a6eb20e33777a46c8657e429c930.jpg\n",
            "Moved: LightFeather-2-_bmp.rf.9cea636ac30984218835a3e399ed3ea3.jpg\n",
            "Moved: BrokenSmall-17-_bmp.rf.1dc4434151106a24f47078cbdef234c6.jpg\n",
            "Moved: White-9-_bmp.rf.903fd9cdbe0a42bf7187d5a933852662.jpg\n",
            "Moved: Triangular-3-_bmp.rf.60e3547ec4753339dd4a9013145921a5.jpg\n",
            "Moved: Bone-12-_bmp.rf.af959dfac9d0761d17bd702fb755e7c3.jpg\n",
            "Moved: SizeA-12-_bmp.rf.fad04fd5e870d712f04449aa75a6a6a4.jpg\n",
            "Moved: SizeA-3-_bmp.rf.fd0255d1927810b7d4123a1ffb8d679a.jpg\n",
            "Moved: SizeB-3-_bmp.rf.9b7d6785f59ab8780ebb143476074f94.jpg\n",
            "Moved: White-4-_bmp.rf.7adb90b01c3dc50060ae44e03be1446d.jpg\n",
            "Moved: BrokenSmall-27-_bmp.rf.88bc3859e3541cf80396dc4484fd3b21.jpg\n",
            "Moved: White-9-_bmp.rf.96b693539591be2a6bd3154a335866ed.jpg\n",
            "Moved: SizeB-11-_bmp.rf.ceb512fb51277610d5f5045862c675a1.jpg\n",
            "Moved: SizeC-11-_bmp.rf.3415a424394a399b8045ed092b2d6e21.jpg\n",
            "Moved: SizeA-9-_bmp.rf.4bb0d6fcd579efd49f34822329c46acb.jpg\n",
            "Moved: Ping-9-_bmp.rf.f81f73e3c17bb7b37021afef348c4e39.jpg\n",
            "Moved: MedFeather-11-_bmp.rf.80ab10c5b966a2f20e0ad133decf0542.jpg\n",
            "Moved: SizeC-5-_bmp.rf.f957a2b24bf660e46c7413354bd30062.jpg\n",
            "Moved: WhiteBeige-11-_bmp.rf.d89ed28f300b9fb2daf05ec9e620dbb1.jpg\n",
            "Moved: BrokenBig-10-_bmp.rf.889d5780aa1c7e6ed1c837a23f7bd76a.jpg\n",
            "Moved: SizeA-12-_bmp.rf.0c315716fa62236f1f9fd36dbbbdc841.jpg\n",
            "Moved: BrokenSmall-29-_bmp.rf.a1c141cb9b4764ae03a7041186d94df5.jpg\n",
            "Moved: WhiteBeige-4-_bmp.rf.9b7cdacb86fd124ce81088c6918209b1.jpg\n",
            "Moved: Beige-13-_bmp.rf.ca541c37857aa6cd169dd3e1a9874c59.jpg\n",
            "Moved: BrokenSmall-2-_bmp.rf.c252dabbdad4f2da725b859b1b986108.jpg\n",
            "Moved: Yellow-8-_bmp.rf.30c2c210058b8c93dc1b905364ec6e8e.jpg\n",
            "Moved: Triangular-2-_bmp.rf.6b9c99b69d07a46a02a5b625107af116.jpg\n",
            "Moved: Triangular-21-_bmp.rf.4509a0e1d86573dfb1e7990080133e2d.jpg\n",
            "Moved: WhiteBeige-4-_bmp.rf.3663a042e025ce6aa65d77ddb65a8a16.jpg\n",
            "Moved: SizeA-4-_bmp.rf.a0b32ecf435f88a620182c14120534ab.jpg\n",
            "Moved: MedFeather-16-_bmp.rf.3214521eb7be95b680805e84f72dbba8.jpg\n",
            "Moved: Yellow-7-_bmp.rf.55ddba002b18b0dac1182ef6340414a2.jpg\n",
            "Moved: Beige-1-_bmp.rf.118573de80b422e6642c42274b0f9df6.jpg\n",
            "Moved: HeavyFeather-5-_bmp.rf.cb1f3cbcf1702a46418c0174e82c68f1.jpg\n",
            "Moved: Triangular-9-_bmp.rf.76ea7d75a35282ae572a343181f4d6c4.jpg\n",
            "Moved: SizeB-14-_bmp.rf.5c81027df9ed012d4f6237dbd11d478a.jpg\n",
            "Moved: SizeC-18-_bmp.rf.b8158f2a58f5fb48bb863acdaf10d840.jpg\n",
            "Moved: BrokenBig-9-_bmp.rf.7a7079abb4d8650b64b22c3e9bfc277d.jpg\n",
            "Moved: HeavyFeather-1-_bmp.rf.d32b4bcb724495e4244b33453f503baa.jpg\n",
            "Moved: BrokenBig-8-_bmp.rf.8e3f9640bdb7291c9a894364b0d0f171.jpg\n",
            "Moved: Strips-12-_bmp.rf.bcd4f4e381b8c7428e214e7157507ad9.jpg\n",
            "Moved: Mossy-1-_bmp.rf.1602c715dd662a19e4514d40615bbf88.jpg\n",
            "Moved: SizeC-15-_bmp.rf.d7ac24cbd049ba2ae480f332d8026a42.jpg\n",
            "Moved: White-1-_bmp.rf.39395c710e687094af1af71f7c909494.jpg\n",
            "Moved: MedFeather-19-_bmp.rf.9fe1463161753ae0b47c3b3496713a19.jpg\n",
            "Moved: Ping-13-_bmp.rf.f09fac6b90183ef3127d3825ecd0e1cb.jpg\n",
            "Moved: BrokenSmall-24-_bmp.rf.c3813b7906075bb0add786d66e20c728.jpg\n",
            "Moved: Bone-2-_bmp.rf.c3ae0fc25d61ec675760a97950659776.jpg\n",
            "Moved: SizeB-7-_bmp.rf.93d317d0ecc6e89925d6754649628497.jpg\n",
            "Moved: Ping-5-_bmp.rf.0a7d9d6d93a0d18043d95dca8905d713.jpg\n",
            "Moved: Mossy-14-_bmp.rf.c6b78a94bfbfb157508b4aa70e4f8832.jpg\n",
            "Moved: White-7-_bmp.rf.dc211d3b87864fc734848e83635d52bc.jpg\n",
            "Moved: Bone-10-_bmp.rf.572ce577bb2d1beacaea1c31346bf7bf.jpg\n",
            "Moved: Yellow-20-_bmp.rf.61bfa1d35eec8f83176f13e64f701fcc.jpg\n",
            "Moved: Triangular-16-_bmp.rf.66e11fbfdbe54501469eb16ca42c9736.jpg\n",
            "Moved: Ping-17-_bmp.rf.3ad83040bc5353d10932d41c209340cf.jpg\n",
            "Moved: Ping-13-_bmp.rf.61dd7d5d25f855530c221a8762d95ba1.jpg\n",
            "Moved: White-7-_bmp.rf.f3216b392cafd498756effc9bb0af6d0.jpg\n",
            "Moved: WhiteBeige-13-_bmp.rf.02b4f35411b5a87224e90ca0be6aaee9.jpg\n",
            "Moved: Bone-4-_bmp.rf.cc3087b5dae22f8b32a0a1a5e4d753ab.jpg\n",
            "Moved: SizeB-8-_bmp.rf.c365fcab65319999bd1fcbc9e8019734.jpg\n",
            "Moved: Triangular-17-_bmp.rf.ce0a77196a7a081e351c6a109269636f.jpg\n",
            "Moved: Yellow-1-_bmp.rf.d6b21cb1fbe2a8158af0ff33132720b9.jpg\n",
            "Moved: MedFeather-1-_bmp.rf.f7ea100f295b9e817fd5056381b69b61.jpg\n",
            "Moved: Yellow-21-_bmp.rf.d0612300d05e93af2000ec91a0cc02f8.jpg\n",
            "Moved: Bone-3-_bmp.rf.7983bff7c47618fea90d07eff4e1611a.jpg\n",
            "Moved: LightFeather-9-_bmp.rf.0958a18cc98c2c3202b718b5310f7d3b.jpg\n",
            "Moved: White-8-_bmp.rf.43b9c4db995f5bd570eb94bb0a010c5f.jpg\n",
            "Moved: MedFeather-11-_bmp.rf.3fd1620d2d83ccae50a7757e3502ca00.jpg\n",
            "Moved: LightFeather-5-_bmp.rf.f7553986563fa735b1b3a4ed2c0a6fc7.jpg\n",
            "Moved: BrokenSmall-25-_bmp.rf.f11380efc37c253c1f30df55202da201.jpg\n",
            "Moved: ThickFlesh-17-_bmp.rf.11cbd75335d5cefd60d5db10bf5b0007.jpg\n",
            "Moved: SizeA-15-_bmp.rf.86795d0f0d0f739d244daaedd8ccbffd.jpg\n",
            "Moved: Mossy-14-_bmp.rf.dccc82346b629de60ab05597c51b596b.jpg\n",
            "Moved: BrokenSmall-31-_bmp.rf.538bf2a0538c210e5cc3f0b1a743b72b.jpg\n",
            "Moved: Beige-9-_bmp.rf.bea86a5ff7584e8a48c2c65d73477fae.jpg\n",
            "Moved: Yellow-15-_bmp.rf.c0062941091e1c02bef27674c700b5cd.jpg\n",
            "Moved: SizeB-7-_bmp.rf.963df17cdf89b67c4280768e9628361d.jpg\n",
            "Moved: Triangular-2-_bmp.rf.a78c6869ac38120ed0dd7ffbee2dc96e.jpg\n",
            "Moved: Shredded-1-_bmp.rf.eac5d0d8642a54e3412a8408a7265ea1.jpg\n",
            "Moved: Beige-1-_bmp.rf.1df5c1329ccc1afe51ba1c345dff8535.jpg\n",
            "Moved: LightFeather-7-_bmp.rf.f5a83805e88683fa942ff1e756218cb4.jpg\n",
            "Moved: HeavyFeather-5-_bmp.rf.4669200c3311fd921513d246616fa361.jpg\n",
            "Moved: Yellow-6-_bmp.rf.b514c261797660fc55166b378c96dddb.jpg\n",
            "Moved: BrokenBig-17-_bmp.rf.3bc118de1cf70c93d06b8843b2baf935.jpg\n",
            "Moved: MedFeather-14-_bmp.rf.83afe01b9d045deeaa885d0881babc87.jpg\n",
            "Moved: WhiteBeige-9-_bmp.rf.b8b5f469d24b4171285a401b81d9fe1c.jpg\n",
            "Moved: ThickFlesh-5-_bmp.rf.57caaa1fae7b9db4e684f5e49aa6c4b7.jpg\n",
            "Moved: White-15-_bmp.rf.1030827bcc217c415a7fe46e12e2f6b3.jpg\n",
            "Moved: Triangular-5-_bmp.rf.c5d5cd5f265acaa4b45ff42e8466ecbe.jpg\n",
            "Moved: Triangular-18-_bmp.rf.db59d7185ed69e6e47482857a2c6db21.jpg\n",
            "Moved: ThickFlesh-5-_bmp.rf.4f17604326dc20f9c0988186e6fb8b62.jpg\n",
            "Moved: Ping-6-_bmp.rf.fea6f3a51ddd892cbbde384a35bc5170.jpg\n",
            "Moved: Bone-8-_bmp.rf.edbc9c37b359b7ddc4eae87ea29b9741.jpg\n",
            "Moved: BrokenBig-9-_bmp.rf.ece1a328857078e0ef6688e7533041cd.jpg\n",
            "Moved: SizeB-15-_bmp.rf.83ce5a79c5b7ca53121ceb61631ba5f2.jpg\n",
            "Moved: Yellow-7-_bmp.rf.ebb39a6f34d454d4278c333a575377bc.jpg\n",
            "Moved: SizeC-16-_bmp.rf.aeb7f575015491bfda163c1a61a6c5f2.jpg\n",
            "Moved: Yellow-21-_bmp.rf.154256856c4bda8872204ecc454f8977.jpg\n",
            "Moved: BrokenSmall-18-_bmp.rf.59f6a910b2aad71c3eb4e50bf0bb6ff7.jpg\n",
            "Moved: Bone-3-_bmp.rf.ac384ce111d44b4a9403970ff3967a38.jpg\n",
            "Moved: MedFeather-15-_bmp.rf.f7e1b1528284db1f4e6076e6c99480a6.jpg\n",
            "Moved: WhiteBeige-15-_bmp.rf.01de2492a2a8322d1107d9f6971ad494.jpg\n",
            "Moved: Ping-15-_bmp.rf.dd0e33c9cc51db7eac9fcfe287d46775.jpg\n",
            "Moved: Yellow-16-_bmp.rf.7be4c69dd690478bead9935e4ce7368a.jpg\n",
            "Moved: Ping-16-_bmp.rf.77841ebb728a9b7deb5f24e7dc0761a2.jpg\n",
            "Moved: HeavyFeather-3-_bmp.rf.36fbaac12ef458fe70f548082d09ee8a.jpg\n",
            "Moved: MedFeather-3-_bmp.rf.54f787c39165c6baafb38da132f40ebf.jpg\n",
            "Moved: BrokenBig-1-_bmp.rf.91974dfc4f86cb150bf0e7b098447ec8.jpg\n",
            "Moved: Bone-9-_bmp.rf.34068b6a66ad901406649a2f6b4a5a2c.jpg\n",
            "Moved: Yellow-1-_bmp.rf.f44cea02909ffde7dea641b775aea7c2.jpg\n",
            "Moved: MedFeather-4-_bmp.rf.0f46f841ed0862509a2beb783c2dc4ac.jpg\n",
            "Moved: Yellow-12-_bmp.rf.24ac4438bbc72616b80989b0863f4ba2.jpg\n",
            "Moved: MedFeather-7-_bmp.rf.10febd7f12aaf21de56315462406a9bd.jpg\n",
            "Moved: ThickFlesh-1-_bmp.rf.ec86597ed402aa51d4b62d5c3bf434af.jpg\n",
            "Moved: White-12-_bmp.rf.30a6ce71271b19a0b1ab752703b85cea.jpg\n",
            "Moved: Mossy-13-_bmp.rf.38cb2a60d41db9159a5150492fa42f1d.jpg\n",
            "Moved: Grey-16-_bmp.rf.764f15f0cf6bd8377db5f4c1d4f2e6b1.jpg\n",
            "Moved: LightFeather-16-_bmp.rf.26d69a8ec0e63f722ab9da1d38a8d98f.jpg\n",
            "Moved: Grey-13-_bmp.rf.5c443df3b02ae0fedde185616c284aea.jpg\n",
            "Moved: MedFeather-17-_bmp.rf.804683815e58a9495c1dd7481467f5b9.jpg\n",
            "Moved: BrokenSmall-71-_bmp.rf.9e4a0dd821427117b7c0f3da9c0a6fc4.jpg\n",
            "Moved: Beige-5-_bmp.rf.d7aaac6550b9a5dc01ea2569f0432e69.jpg\n",
            "Moved: WhiteBeige-2-_bmp.rf.b30d99749b04181a1326e6747e24b4e8.jpg\n",
            "Moved: White-3-_bmp.rf.b0c79aeb13f7b06831e6dfd293278983.jpg\n",
            "Moved: SizeA-6-_bmp.rf.bfa0f7b34fc0d966cb3f3fb93d45f65e.jpg\n",
            "Moved: Yellow-3-_bmp.rf.262f1d4e5bf874b5f8c0652992771a83.jpg\n",
            "Moved: Yellow-18-_bmp.rf.eee9867f8239acc3c1aa591cc3d5f3a5.jpg\n",
            "Moved: Bone-3-_bmp.rf.ef89de9ca20a200385173474f2ae44e8.jpg\n",
            "Moved: WhiteBeige-11-_bmp.rf.da44f4cf1866eaae743e70a8cc8cf77d.jpg\n",
            "Moved: Yellow-7-_bmp.rf.354dc0f848848cbf66694f6d9564fb02.jpg\n",
            "Moved: LightFeather-3-_bmp.rf.c8b5b3cd6b2974060277a6b6e9f80905.jpg\n",
            "Moved: SizeC-15-_bmp.rf.a2a9d32d9ee8b86f048a5e3aa75f443e.jpg\n",
            "Moved: DoubleColour-2-_bmp.rf.d2c91b77871f763ca5c2acc8b59a5585.jpg\n",
            "Moved: SizeC-14-_bmp.rf.5166c92a9820240808482a93392b8937.jpg\n",
            "Moved: Ping-14-_bmp.rf.1d955383b41cc0645cb4beee4f0c9039.jpg\n",
            "Moved: MedFeather-20-_bmp.rf.b0f7cfda0c72069265edc9c52fa92856.jpg\n",
            "Moved: Triangular-6-_bmp.rf.617041ce89bbd79945cad65740bfc1fd.jpg\n",
            "Moved: BrokenBig-16-_bmp.rf.0e5b8e972d204d5e4276d200ee8ac3e1.jpg\n",
            "Moved: SizeC-11-_bmp.rf.072d98be7b5b684eefdd7f156c952aec.jpg\n",
            "Moved: SizeC-9-_bmp.rf.9a793f399e8b8ddfa98f57380cbbd14e.jpg\n",
            "Moved: Yellow-4-_bmp.rf.720bb0c7927cf757ddbb6e30de3658cd.jpg\n",
            "Moved: Triangular-17-_bmp.rf.e77d6ca37fd0c1a862bb9c571f4fccbe.jpg\n",
            "Moved: WhiteBeige-7-_bmp.rf.c1344bcd5ef74c19f4d49d1018dc40de.jpg\n",
            "Moved: WhiteBeige-5-_bmp.rf.87b53d5fd024777ccc5685a9dc810203.jpg\n",
            "Moved: SizeC-12-_bmp.rf.38774cff03a2f22d256b218218836ddb.jpg\n",
            "Moved: SizeC-12-_bmp.rf.a9fd550480f3596f1a5c7ed22002cecf.jpg\n",
            "Moved: Ping-9-_bmp.rf.d5490d742e631fca4d3cd23078437f95.jpg\n",
            "Moved: Ping-12-_bmp.rf.3fea271351b635090d08485b4eb7bf35.jpg\n",
            "Moved: Yellow-3-_bmp.rf.51ae3d896f23fb55bcefd07d83123a0a.jpg\n",
            "Moved: HeavyFeather-5-_bmp.rf.c5e977082e49bf9cac0cfa8a47f16b43.jpg\n",
            "Moved: SizeC-1-_bmp.rf.4927a47044dd0622cf4dd0af6d0936c3.jpg\n",
            "Moved: WhiteBeige-14-_bmp.rf.660b3908bacbaf059c2b982e48969f63.jpg\n",
            "Moved: Beige-8-_bmp.rf.f528f45e6987413184cde275ecd9cee2.jpg\n",
            "Moved: WhiteBeige-10-_bmp.rf.b310c5181ca11d39df906ff96ea88232.jpg\n",
            "Moved: SizeB-15-_bmp.rf.aefa495df3ed4d70c5f181bf3fc71f9a.jpg\n",
            "Moved: BrokenSmall-32-_bmp.rf.e724654a7f48cde7c36e450ab76e451c.jpg\n",
            "Moved: SizeA-11-_bmp.rf.36c65c3fbfaefbd3aca1a9d9e194e773.jpg\n",
            "Moved: SizeB-5-_bmp.rf.95ba6ab06341e4f4eac3e829e90bed20.jpg\n",
            "Moved: SizeA-9-_bmp.rf.b2116c5c5783e9d0725728d250ca6e08.jpg\n",
            "Moved: Triangular-24-_bmp.rf.8f7db0ed432000b483caa50a2049554a.jpg\n",
            "Moved: Yellow-13-_bmp.rf.1746c75ac24eece7e8149ef00f99c5c5.jpg\n",
            "Moved: Grey-13-_bmp.rf.38679cc38e3cb18257573c0fb8c3cddc.jpg\n",
            "Moved: LightFeather-2-_bmp.rf.d56e04100ba811f5750c3a253d67735a.jpg\n",
            "Moved: Beige-10-_bmp.rf.94e2dee505ad1154510b0a62a36a1043.jpg\n",
            "Moved: MedFeather-15-_bmp.rf.9f69b860c2e37ee28bd549c833a25659.jpg\n",
            "Moved: White-12-_bmp.rf.ed4017b2c4314f42b03900e6ca98f989.jpg\n",
            "Moved: WhiteBeige-16-_bmp.rf.c5db2747ce635acb5f867a2616c26c90.jpg\n",
            "Moved: WhiteBeige-14-_bmp.rf.9ed29e406109e7a47a3695369e096b1a.jpg\n",
            "Moved: LightFeather-3-_bmp.rf.1b5d15cb1f95c9f215ebc45c6d5485bc.jpg\n",
            "Moved: BrokenBig-17-_bmp.rf.0123d52adcfa0d6af9d9446de5448182.jpg\n",
            "Moved: White-16-_bmp.rf.7062b7d6596fc469481dbf2ff9eb5ce9.jpg\n",
            "Moved: Bone-8-_bmp.rf.2597a47088db3f6ff0c2bf4b1658d6f4.jpg\n",
            "Moved: Bone-11-_bmp.rf.9a57b87bb80a11abed667c9c3a055951.jpg\n",
            "Moved: LightFeather-9-_bmp.rf.1c0716adc13ebabd511c4391fa6241b0.jpg\n",
            "Moved: Strips-10-_bmp.rf.18bf07e84bcb54dbe668a4a9a00e7d19.jpg\n",
            "Moved: SizeA-10-_bmp.rf.1e999815206c3611ddb0f8e3c6c30d4a.jpg\n",
            "Moved: BrokenBig-10-_bmp.rf.dd0749fc304282ef05373a5076500648.jpg\n",
            "Moved: LightFeather-10-_bmp.rf.6a640d7d12191b9102be50847addf9aa.jpg\n",
            "Moved: ThickFlesh-13-_bmp.rf.b6413288813372b21283afa274262b23.jpg\n",
            "Moved: SizeC-8-_bmp.rf.638e78a72e55b11f56126fb3138738df.jpg\n",
            "Moved: BrokenBig-1-_bmp.rf.79ceea0265eec72f55902174e83acb35.jpg\n",
            "Moved: SizeC-8-_bmp.rf.a0da8060c74370d648641a6502e907ff.jpg\n",
            "Moved: Mossy-13-_bmp.rf.1c51f26269594097294e80e8d466bb19.jpg\n",
            "Moved: Beige-1-_bmp.rf.ce480e99fa3044b4b26f841654933876.jpg\n",
            "Moved: Triangular-20-_bmp.rf.36651a81dde13327221db68f8ab9750e.jpg\n",
            "Moved: SizeC-4-_bmp.rf.562cdf18d3c86711f8c1bfbc014671b7.jpg\n",
            "Moved: BrokenSmall-11-_bmp.rf.29e3c978426f15694ce8e917de45d735.jpg\n",
            "Moved: Mossy-6-_bmp.rf.0239624a2731094eb6a253faf3fc46de.jpg\n",
            "Moved: LightFeather-19-_bmp.rf.38b4ca49eb67489355bae9fc297ab343.jpg\n",
            "Moved: BrokenSmall-37-_bmp.rf.747c8ae96f276cf0fedb71a638f25ea1.jpg\n",
            "Moved: DaYuenJiao-7-_bmp.rf.418954f33481e94bf4ac7312fee07be4.jpg\n",
            "Moved: SizeC-3-_bmp.rf.21bbdc7879158ad8d95500063a398059.jpg\n",
            "Moved: Triangular-3-_bmp.rf.94944a298fc6f54932d98a999366d7c0.jpg\n",
            "Moved: Triangular-22-_bmp.rf.ee91300b802ab25b43b0ccd82abba460.jpg\n",
            "Moved: Triangular-18-_bmp.rf.1761e50103dc4152ee239d555ba28946.jpg\n",
            "Moved: Yellow-12-_bmp.rf.9de9cd20a3335398f26dcdc5d57dcbe2.jpg\n",
            "Moved: SizeC-18-_bmp.rf.d20ee923836d3650e16b15c32d62173a.jpg\n",
            "Moved: DaYuenJiao-7-_bmp.rf.396367d810b6a44a53769d57e052c560.jpg\n",
            "Moved: SizeB-9-_bmp.rf.c12dcea383d0485260fe2f6a7c8f42f2.jpg\n",
            "Moved: Mossy-3-_bmp.rf.08af738a79e409577b64d31707db7b9c.jpg\n",
            "Moved: BrokenBig-5-_bmp.rf.cf771decc90320877077d4f9dee8a683.jpg\n",
            "Moved: BrokenSmall-26-_bmp.rf.3a406ef9f9cfd6076771e3ee49e7d66b.jpg\n",
            "Moved: Yellow-12-_bmp.rf.5a47a0cceb8a6af01a75fbe2d86e970e.jpg\n",
            "Moved: BrokenSmall-34-_bmp.rf.a178712e2ed07519b57e6bebeaab94b1.jpg\n",
            "Moved: Strips-6-_bmp.rf.49d3f77d5db9d83b681101240f56c39f.jpg\n",
            "Moved: SizeB-13-_bmp.rf.7fc0e0c78fc854f8bb5af8379a968a49.jpg\n",
            "Moved: Bone-1-_bmp.rf.0d41f2a668afb723741971950767b259.jpg\n",
            "Moved: ThickFlesh-14-_bmp.rf.f7be80693fffd99d64ca5823952883db.jpg\n",
            "Moved: BrokenSmall-30-_bmp.rf.5556548d085a2cb0697961e388f73a76.jpg\n",
            "Moved: Ping-17-_bmp.rf.819e2ad09e4598c3aaefd5dc9c6fde2b.jpg\n",
            "Moved: Beige-2-_bmp.rf.ccabf640c3a8b7432fa459f7f302b85c.jpg\n",
            "Moved: LightFeather-18-_bmp.rf.f31b04b822ebf86fda6a6eab0e27cbaf.jpg\n",
            "Moved: BrokenBig-9-_bmp.rf.be12a09d6a1295c4b8afb305dee9a921.jpg\n",
            "Moved: White-17-_bmp.rf.feee73f36b3bcbd981def634bc478cae.jpg\n",
            "Moved: Ping-3-_bmp.rf.d918ebfdebbc3ab133c163cc848e85ac.jpg\n",
            "Moved: DoubleColour-3-_bmp.rf.e676aba08eae164f63e16cf61f41c7ae.jpg\n",
            "Moved: BrokenSmall-24-_bmp.rf.9835b7317fcc9a96dd8942db5144e826.jpg\n",
            "Moved: WhiteBeige-17-_bmp.rf.23b3a690527e336dac127b674c9d38d4.jpg\n",
            "Moved: LightFeather-14-_bmp.rf.833891bc58c622e3b70eb45eafe717c8.jpg\n",
            "Moved: Triangular-2-_bmp.rf.f809d106bfb7d5d4ab459309203cd07c.jpg\n",
            "Moved: LightFeather-18-_bmp.rf.c714b351dd73115b519dd245ccc65502.jpg\n",
            "Moved: Triangular-16-_bmp.rf.9748048c1b70a84136967eed87e93707.jpg\n",
            "Moved: Bone-2-_bmp.rf.8c0db373db5446e23a2c1982582be869.jpg\n",
            "Moved: MedFeather-3-_bmp.rf.2196b892a4aefd8638fb18f35e2c90f2.jpg\n",
            "Moved: SizeC-15-_bmp.rf.06348c6c7347f8bd7414a5d4b2979aca.jpg\n",
            "Moved: Strips-10-_bmp.rf.eae97ceba83d562db3c4626c180a5a4d.jpg\n",
            "Moved: Yellow-18-_bmp.rf.1a5898cf5d818bb8c3782a0fba340f41.jpg\n",
            "Moved: BrokenBig-12-_bmp.rf.fa7bc654c4f6b70065d354c9cf75d520.jpg\n",
            "Moved: SizeB-3-_bmp.rf.f2279232ad3c39b3c49d2c4075a875d1.jpg\n",
            "Moved: Beige-6-_bmp.rf.f40b9f97cb61f9ffb7c549b8f05be647.jpg\n",
            "Moved: White-2-_bmp.rf.20ec9e178b4da0790fa8424b16b63c93.jpg\n",
            "Moved: Grey-17-_bmp.rf.93c9045763e8c761dcdd76b089b1f085.jpg\n",
            "Moved: Ping-5-_bmp.rf.32f65bb9b4c538244678ddadaef31f27.jpg\n",
            "Moved: SizeA-8-_bmp.rf.88c9e624247d86ef35cd8a431a8eb58b.jpg\n",
            "Moved: Bone-6-_bmp.rf.c50553ed54b526fb4f6b92e1bdd0e44c.jpg\n",
            "Moved: Mossy-7-_bmp.rf.7c71786781f9a9d2519f795d4dd05652.jpg\n",
            "Moved: SizeC-4-_bmp.rf.c1ec154aad018b883a519fc850ed3bd5.jpg\n",
            "Moved: Triangular-19-_bmp.rf.5030d0c674f56b18656baa5c256b54c9.jpg\n",
            "Moved: SizeB-5-_bmp.rf.671e591dc8f1cedb51728778ee45f98e.jpg\n",
            "Moved: MedFeather-16-_bmp.rf.255102aa83ca1fe63409fd585b13fb16.jpg\n",
            "Moved: HeavyFeather-1-_bmp.rf.6c87e6736ee20d495c8793afa836fac1.jpg\n",
            "Moved: BrokenBig-19-_bmp.rf.81bf8e7f0755b9b655107ad12ca394f7.jpg\n",
            "Moved: White-8-_bmp.rf.530b11798f522a13c7e4fd465fac9f04.jpg\n",
            "Moved: SizeC-3-_bmp.rf.8a662b8f4753cdf5eb1e6ae4c9d7a9b8.jpg\n",
            "Moved: WhiteBeige-5-_bmp.rf.d85e71a6364f925d9d8396f3a420dd1a.jpg\n",
            "Moved: Mossy-1-_bmp.rf.529555aeb644f15a8362a5bbb740ca82.jpg\n",
            "Moved: White-12-_bmp.rf.0bff1d9c50e879bd84f01b8c9aaf473e.jpg\n",
            "Moved: LightFeather-19-_bmp.rf.b9da2710e29c32bf32f84434fc817fd0.jpg\n",
            "Moved: Ping-11-_bmp.rf.c8e09cbca7cdeb5946a8ef061b8cabf9.jpg\n",
            "Moved: Strips-9-_bmp.rf.783e4638feb0ea3a201f88c562b8f300.jpg\n",
            "Moved: Beige-8-_bmp.rf.840fd4eaeb1adff21b16c7f12859aa0a.jpg\n",
            "Moved: Yellow-18-_bmp.rf.b6503d463cd24068212019e1be58ad9f.jpg\n",
            "Moved: Beige-11-_bmp.rf.d9c1067065982fe16292f25bf80ddc59.jpg\n",
            "Moved: Yellow-6-_bmp.rf.c7fcd8989c7fb79b800e7643e35d55b7.jpg\n",
            "Moved: Yellow-17-_bmp.rf.bedfe897d82fc4c243a2370c560b6926.jpg\n",
            "Moved: Ping-6-_bmp.rf.540b34c9c86b16105ca46101d2047c4e.jpg\n",
            "Moved: Triangular-11-_bmp.rf.4063f2eae1de5d2c28ca39f3b87e3f9e.jpg\n",
            "Moved: BrokenBig-7-_bmp.rf.de798a062f72f063b9399614297c2a10.jpg\n",
            "Moved: BrokenSmall-2-_bmp.rf.fbe79e8e448d00fd40d1ce1da0cd4fc3.jpg\n",
            "Moved: Ping-6-_bmp.rf.b493bf1edcef7d5c15dcbd37c723cb6e.jpg\n",
            "Moved: Bone-1-_bmp.rf.bc58860ce574cfb1d3af95d0abb7e136.jpg\n",
            "Moved: Beige-11-_bmp.rf.539d709dbb4bea76a94df87a24e66058.jpg\n",
            "Moved: LightFeather-7-_bmp.rf.049cc5eb588fca94a3cd3fdc29cdc652.jpg\n",
            "Moved: BrokenSmall-27-_bmp.rf.8f952a3e833cfa4ad83d9e95b5baf2ba.jpg\n",
            "Moved: Grey-16-_bmp.rf.0fca81bcaccfd46dacd413042b0a1a1a.jpg\n",
            "Moved: BrokenSmall-71-_bmp.rf.5f218661b5b91f5225247f37490d9577.jpg\n",
            "Moved: White-4-_bmp.rf.dd30ace1661e3ffca85271f9e7384183.jpg\n",
            "Moved: Grey-16-_bmp.rf.3a8bf7509489435d08e6f2ef471e7ca7.jpg\n",
            "Moved: Yellow-19-_bmp.rf.4fad73da3aab6b15c847afa3ead73731.jpg\n",
            "Moved: HeavyFeather-3-_bmp.rf.be632056c773518f494360020cdd8f50.jpg\n",
            "Moved: Beige-12-_bmp.rf.70d8e3315563bc5ee62190002fe4b731.jpg\n",
            "Moved: SizeA-13-_bmp.rf.236c74c3f8f34835cbc96cce915805eb.jpg\n",
            "Moved: Bone-12-_bmp.rf.26e2f41f1b28adc5b24ece0a6e983b05.jpg\n",
            "Moved: White-11-_bmp.rf.e9e6753d819a326b8e1f5ad8a79cfa41.jpg\n",
            "Moved: DoubleColour-2-_bmp.rf.b112eacb6a59223fa027b9158dd9dd6b.jpg\n",
            "Moved: SizeA-2-_bmp.rf.2c9cc8e0453593df79fef7d5c92afe1b.jpg\n",
            "Moved: Beige-7-_bmp.rf.4d6e6dcf6139bf262750cd9d905a0165.jpg\n",
            "Moved: White-18-_bmp.rf.273f2dd2685dd73b8b22affbbf93accb.jpg\n",
            "Moved: Yellow-11-_bmp.rf.6c66ac27aefb391a06219ee2260ac0a5.jpg\n",
            "Moved: Ping-15-_bmp.rf.0372a344db3a3bfe6e5fbd8a8f04305f.jpg\n",
            "Moved: MedFeather-14-_bmp.rf.baccd5f116b078e2d8bd6f90a3f51af7.jpg\n",
            "Moved: Mossy-11-_bmp.rf.f34a5f51efd9ae70250ac2c73d4c10cb.jpg\n",
            "Moved: DoubleColour-2-_bmp.rf.e9e8c5c27f55e9da62617296ef10b935.jpg\n",
            "Moved: Triangular-18-_bmp.rf.c19fa0acb9c381c12eef149089e974d1.jpg\n",
            "Moved: Ping-17-_bmp.rf.2ddaf18ee7c82398f550e077895e81b0.jpg\n",
            "Moved: BrokenSmall-24-_bmp.rf.519c41ec724a2c6ee5ba2b08559d4d04.jpg\n",
            "Moved: WhiteBeige-4-_bmp.rf.b99b71664f1ec542467784be97a04b2b.jpg\n",
            "Moved: HeavyFeather-3-_bmp.rf.0c51b165b15ba1722cd97bf31fcb3900.jpg\n",
            "Moved: HeavyFeather-10-_bmp.rf.7858febbda1e6c908f97b50b8c1a9e44.jpg\n",
            "Moved: Strips-12-_bmp.rf.4d189469b7be280d2c22798162593aff.jpg\n",
            "Moved: Strips-18-_bmp.rf.bc10d9f3f443ff65068d1423495fac92.jpg\n",
            "Moved: White-13-_bmp.rf.973f1732c72d55648d3cc684e06ff2f3.jpg\n",
            "Moved: BrokenSmall-21-_bmp.rf.89847bf4179bf460a0d59d181e22607a.jpg\n",
            "Moved: ThickFlesh-6-_bmp.rf.ce872597a1f9cf764c5fd4ed89f2cf56.jpg\n",
            "Moved: Ping-2-_bmp.rf.3f43d5ef3dfbaec8b2bb60e3ef1262da.jpg\n",
            "Moved: SizeB-10-_bmp.rf.0f34b3c5558bc3de203cdc910ec59cd9.jpg\n",
            "Moved: Beige-12-_bmp.rf.c56b5229d1967dd7510608a329a600ad.jpg\n",
            "Moved: BrokenSmall-25-_bmp.rf.72288fd13cfd815b7283bcb8fa7869f7.jpg\n",
            "Moved: Yellow-16-_bmp.rf.42ab18f252b65c7898be9d630ecc09b7.jpg\n",
            "Moved: MedFeather-19-_bmp.rf.8384f8ae3d9703fca10ad15e675820a7.jpg\n",
            "Moved: MedFeather-17-_bmp.rf.709167ee38745c3f3962cf4793b15759.jpg\n",
            "Moved: SizeB-7-_bmp.rf.b3fd3bcdec80ca5b74a57aa45f499ea4.jpg\n",
            "Moved: LightFeather-17-_bmp.rf.56b72dda98e14bf1da09059fdddf1971.jpg\n",
            "Moved: BrokenSmall-17-_bmp.rf.d6020d8dda785300a72043bb263dd46f.jpg\n",
            "Moved: MedFeather-4-_bmp.rf.85b4bdeea254381ce8cae329245c5440.jpg\n",
            "Moved: SizeA-10-_bmp.rf.33f1cfa970e48a3a245955cf60f10125.jpg\n",
            "Moved: SizeC-11-_bmp.rf.6e33b10f327fa8a5407dba69825e81c4.jpg\n",
            "Moved: SizeA-9-_bmp.rf.dc5c984b463f434fc5615f5430a01bc3.jpg\n",
            "Moved: BrokenSmall-21-_bmp.rf.dd1c1dae5f83c0e17216bf4b5439314a.jpg\n",
            "Moved: SizeB-6-_bmp.rf.7f8b45635aad6ac366ab35dfddfbabb6.jpg\n",
            "Moved: WhiteBeige-8-_bmp.rf.3b035c4ef1aa285ff8efd40b67865706.jpg\n",
            "Moved: SizeC-7-_bmp.rf.02ae3dadc39296f545102a2fca1f5627.jpg\n",
            "Moved: Shredded-1-_bmp.rf.46ca11691db4474392e1b7f6e92ba99b.jpg\n",
            "Moved: WhiteBeige-2-_bmp.rf.ad92a50fb54e482776677974e81dd05a.jpg\n",
            "Moved: WhiteBeige-3-_bmp.rf.7e7df3f83dbb4cc67d82a1e567b8b797.jpg\n",
            "Moved: ThickFlesh-3-_bmp.rf.83db9985077379b6cde42d03f0d2f785.jpg\n",
            "Moved: BrokenBig-13-_bmp.rf.9c5183059ff9693180e528f459490ede.jpg\n",
            "Moved: BrokenBig-11-_bmp.rf.c59bd267442b11c384da955cc608170d.jpg\n",
            "Moved: WhiteBeige-11-_bmp.rf.110b8caadaff1558f8497baa08631ecc.jpg\n",
            "Moved: ThickFlesh-17-_bmp.rf.542663d7706872bbbe5e675ddee5b436.jpg\n",
            "Moved: SizeA-2-_bmp.rf.7f6839f4e7dbacb4d6affd0ad62acaf5.jpg\n",
            "Moved: MedFeather-1-_bmp.rf.a338c75c2e29b5495e47197589cac7c6.jpg\n",
            "Moved: Triangular-12-_bmp.rf.701cd5f19b8dcf61e721d194cadb53a8.jpg\n",
            "Moved: Yellow-5-_bmp.rf.e987564a3b362e25076bf95850a7f375.jpg\n",
            "Moved: SizeC-13-_bmp.rf.4542bf6a7664afe35a0fc1ed01274ed8.jpg\n",
            "Moved: BrokenSmall-30-_bmp.rf.502f807ec133715bcefae43cbce1e514.jpg\n",
            "Moved: Beige-5-_bmp.rf.cfed1568e4233251aaef8896bcb11c6d.jpg\n",
            "Moved: ThickFlesh-3-_bmp.rf.59d23703150f57e24398caf658153890.jpg\n",
            "Moved: White-2-_bmp.rf.0d20a698aad4a8a6628e817c333b0901.jpg\n",
            "Moved: Beige-12-_bmp.rf.92b8457edcf7f87d4b7cb1fc97113b48.jpg\n",
            "Moved: Triangular-4-_bmp.rf.051c9361e68c342feb06306d6d4d2642.jpg\n",
            "Moved: Yellow-13-_bmp.rf.0b25606dd47dffb17390ab22be06bc72.jpg\n",
            "Moved: SizeB-13-_bmp.rf.d1e4c697ca87cee8e8de1df6ab24ea05.jpg\n",
            "Moved: White-15-_bmp.rf.8a2685262bb59633dddcaec991da9aa3.jpg\n",
            "Moved: SizeB-14-_bmp.rf.61f66535ff062f54f956cc96cf3ad523.jpg\n",
            "Moved: Bone-13-_bmp.rf.ed983ab6d15d649053d624775242e51f.jpg\n",
            "Moved: White-17-_bmp.rf.44ad17cfcacc988b93797a17661234ed.jpg\n",
            "Moved: BrokenSmall-14-_bmp.rf.da921da22967d9962d6e33a3428c44f5.jpg\n",
            "Moved: BrokenSmall-13-_bmp.rf.35a49bcab3e58c23e648b37590cdc1df.jpg\n",
            "Moved: BrokenSmall-13-_bmp.rf.aa7ef06af8f79f39960b92d897f8ad13.jpg\n",
            "Moved: ThickFlesh-11-_bmp.rf.60402669fd541d85b275fcde3370b16f.jpg\n",
            "Moved: SizeB-9-_bmp.rf.5633c87216420efb0875457b6bb0043b.jpg\n",
            "Moved: Yellow-20-_bmp.rf.b34a40dbe75669cac0c1e5c50a503f32.jpg\n",
            "Moved: BrokenSmall-29-_bmp.rf.fa5acdd7107e24f7a195297526d08e2c.jpg\n",
            "Moved: BrokenBig-5-_bmp.rf.ed1889a090d23c28c49eb449d9f9a521.jpg\n",
            "Moved: SizeA-1-_bmp.rf.9bf76434a34532a094bbf1720f1c699c.jpg\n",
            "Moved: HeavyFeather-7-_bmp.rf.f67a1148c12425fe8eea7739da34d5dd.jpg\n",
            "Moved: Triangular-24-_bmp.rf.33f7384b6b5503d95d1b25d5cf9b22cc.jpg\n",
            "Moved: Bone-13-_bmp.rf.4938a31bdf66ddffff25821492b9a150.jpg\n",
            "Moved: WhiteBeige-16-_bmp.rf.a8f6f4963126e3a942020ce016d993d1.jpg\n",
            "Moved: Yellow-4-_bmp.rf.13216d621f8f90ca8ca273c78cc22628.jpg\n",
            "Moved: Yellow-17-_bmp.rf.9395aa30a694d10db192d4bababaff84.jpg\n",
            "Moved: MedFeather-13-_bmp.rf.acc6b2eb010cd13309604357b2b2f4d0.jpg\n",
            "Moved: WhiteBeige-7-_bmp.rf.2ad73b66030014ad8982bb38edac948e.jpg\n",
            "Moved: SizeC-7-_bmp.rf.be274bc197c4d218ed9a3543bdf52ab3.jpg\n",
            "Moved: LightFeather-16-_bmp.rf.347b020d79981fa534c172c2c5d5d1f7.jpg\n",
            "Moved: Grey-17-_bmp.rf.fce4efae748982930ce59c47dd109bca.jpg\n",
            "Moved: White-3-_bmp.rf.9620cf3c60d35745c3181e986834bc91.jpg\n",
            "Moved: MedFeather-13-_bmp.rf.e4ecf158cc62b3170ccf775f4fc4f673.jpg\n",
            "Moved: White-9-_bmp.rf.a7a2afcfb935cec9a49787f50221ac63.jpg\n",
            "Moved: LightFeather-14-_bmp.rf.d1732abbb5bc26767097cbcc532cd4ff.jpg\n",
            "Moved: LightFeather-8-_bmp.rf.986c310425b681dc3c1735e1c1839da4.jpg\n",
            "Moved: WhiteBeige-10-_bmp.rf.3f4beba00f3fb4ba59633eab6d83e117.jpg\n",
            "Moved: Beige-1-_bmp.rf.e25b5dfda7a964e605a8048d8a9a5d84.jpg\n",
            "Moved: Triangular-23-_bmp.rf.17349acf962e690dcfd59b8b84fd86c1.jpg\n",
            "Moved: Beige-1-_bmp.rf.1c9dfe94b2a5830d3f025aa1669712c1.jpg\n",
            "Moved: Bone-6-_bmp.rf.bf05d6dc19a2d68b03af4c940394447a.jpg\n",
            "Moved: Triangular-12-_bmp.rf.c306f2b59eefafc9a91459b7fbd2e2d9.jpg\n",
            "Moved: White-2-_bmp.rf.2d93acc68cbecc7a33df1909900b59df.jpg\n",
            "Moved: Triangular-24-_bmp.rf.461d8badfba7e886b7274f6dcdddcb56.jpg\n",
            "Moved: WhiteBeige-17-_bmp.rf.bc9facc76534dcf68797d1ebdad7c1b8.jpg\n",
            "Moved: SizeC-2-_bmp.rf.45fb1eefdbb488a7637322123881af73.jpg\n",
            "Moved: Ping-10-_bmp.rf.93a2da00c23c20ce233a7e72b73de14e.jpg\n",
            "Moved: SizeB-1-_bmp.rf.f4ef5c50875b26bc793e5891864e6cbb.jpg\n",
            "Moved: ThickFlesh-7-_bmp.rf.7a791ec1219fd4039fe000304d1903e4.jpg\n",
            "Moved: HeavyFeather-9-_bmp.rf.cf0235fe710d8bb19726a76ba13a89ae.jpg\n",
            "Moved: Triangular-4-_bmp.rf.ba6fe02489af804b160deab95abbffd6.jpg\n",
            "Moved: SizeC-1-_bmp.rf.cc3a47ccdd237b87931eed80fd20bafd.jpg\n",
            "Moved: SizeB-6-_bmp.rf.8f5bc0144c6b7cc9db0f48cb61a8f16d.jpg\n",
            "Moved: ThickFlesh-17-_bmp.rf.0d2dce40f7f9f98d72a6748d9bf702f4.jpg\n",
            "Moved: SizeA-15-_bmp.rf.6994500144e1295ab300f6404b2d8780.jpg\n",
            "Moved: BrokenSmall-22-_bmp.rf.18513dd80232933496ddd294efa494ce.jpg\n",
            "Moved: Beige-6-_bmp.rf.f112fe0e53d57677a2ac5e9935582c38.jpg\n",
            "Moved: Ping-9-_bmp.rf.3631bd7c6975b37172c015a9b38548e1.jpg\n",
            "Moved: Grey-24-_bmp.rf.3830d795f3d8ca15edce06631601d8a8.jpg\n",
            "Moved: Yellow-21-_bmp.rf.07d46563433febb87aa44c6c2cda0b2d.jpg\n",
            "Moved: Strips-6-_bmp.rf.e25b8a96e335cca7002cd7c1d17f2585.jpg\n",
            "Moved: SizeC-17-_bmp.rf.9873372230f339ab79c8b2e020b48aad.jpg\n",
            "Moved: SizeB-11-_bmp.rf.f9b018435be081d460eeed7fa8787885.jpg\n",
            "Moved: SizeB-1-_bmp.rf.019ab413cb9acf99f76a848508d5cf52.jpg\n",
            "Moved: BrokenSmall-31-_bmp.rf.43881ef765a3558ada45210a6f566d2f.jpg\n",
            "Moved: Triangular-3-_bmp.rf.490fd77c2c4f1d6f0641d80d233f3e7b.jpg\n",
            "Moved: MedFeather-14-_bmp.rf.e1c5ba4e217ee641b6c59e52acd19c6b.jpg\n",
            "Moved: BrokenBig-16-_bmp.rf.f39d5efae5aee4e1ccc2bad221ee218e.jpg\n",
            "Moved: Yellow-4-_bmp.rf.43a6e8e8fc2912111b3c6160e5e98392.jpg\n",
            "Moved: SizeC-2-_bmp.rf.2f5c48249312322e2c594c3994258847.jpg\n",
            "Moved: WhiteBeige-15-_bmp.rf.8941dc1b87c3f93c984620759183243f.jpg\n",
            "Moved: White-17-_bmp.rf.46480c1db452ab9ba1e9f8985857b8a3.jpg\n",
            "Moved: BrokenSmall-26-_bmp.rf.4618ea858d2111b6ff4f0d1ea453ef9f.jpg\n",
            "Moved: Beige-3-_bmp.rf.99ea6d358721a0f77d9b17f1677a9ba3.jpg\n",
            "Moved: SizeA-1-_bmp.rf.57f5790d3b44c85616108efa4e1d2262.jpg\n",
            "Moved: SizeA-11-_bmp.rf.166c7e819f092f673fbb9ba2ebcaeed3.jpg\n",
            "Moved: BrokenBig-15-_bmp.rf.a4cf2b53d9fd114ab9801dee13e018e1.jpg\n",
            "Moved: Triangular-19-_bmp.rf.7a50f369d8b72c168674ab6edd286b38.jpg\n",
            "Moved: Beige-2-_bmp.rf.78cf247cba98e943f6c3c89220777952.jpg\n",
            "Moved: ThickFlesh-11-_bmp.rf.e9f1cf3960808386cf2e345c3c03c232.jpg\n",
            "Moved: SizeB-6-_bmp.rf.8ad17e1b723cd2f21ad1f973143d992f.jpg\n",
            "Moved: Mossy-6-_bmp.rf.4ee8973da1d47c4292374be3753a39ac.jpg\n",
            "Moved: SizeB-13-_bmp.rf.007032ebf1cc61452d9c00bc37be10b6.jpg\n",
            "Moved: BrokenSmall-11-_bmp.rf.f3f639d8742567baf767d959cd65be0c.jpg\n",
            "Moved: WhiteBeige-13-_bmp.rf.826023eadeed3fa95cb1e271a982ee21.jpg\n",
            "Moved: Beige-13-_bmp.rf.2e7b74e18f4e3e4f2dd2dc5c614b1cd5.jpg\n",
            "Moved: Bone-7-_bmp.rf.db406da2aa1804c169d0afecc026de98.jpg\n",
            "Moved: Mossy-3-_bmp.rf.5a099eaa34e151bf91a4331ef17f9b52.jpg\n",
            "Moved: LightFeather-10-_bmp.rf.91270d890337bc180c82bf4444b2a661.jpg\n",
            "Moved: MedFeather-20-_bmp.rf.c87b531713aa46f76e28090386dbec9a.jpg\n",
            "Moved: BrokenSmall-2-_bmp.rf.900ded07f8b4f25a4b4166ce2a4b4a8c.jpg\n",
            "Moved: Beige-3-_bmp.rf.87f0363c71680ecd0d09bcdee3c50d0e.jpg\n",
            "Moved: SizeC-18-_bmp.rf.a9afa1c49934748136528b536572af20.jpg\n",
            "Moved: Yellow-16-_bmp.rf.34bddc48ba425f5e1737ae787a691363.jpg\n",
            "Moved: White-13-_bmp.rf.8b0b6005d523ab52833f0c60810a7fe1.jpg\n",
            "Moved: White-18-_bmp.rf.a5673774ded9b17a8c1cd4d83eddfb71.jpg\n",
            "Moved: Bone-7-_bmp.rf.bb54cf7c83d203d51a0a07f07b12f02b.jpg\n",
            "Moved: BrokenBig-11-_bmp.rf.21caae9fef809d1188afa85e9b9b6ef5.jpg\n",
            "Moved: SizeA-8-_bmp.rf.d717f01249068accd504340b83cba334.jpg\n",
            "Moved: ThickFlesh-11-_bmp.rf.226450e5692f76ae8f0c3e63831419b8.jpg\n",
            "Moved: SizeC-9-_bmp.rf.2933f1454ae49a4eda478d3c07ba9e06.jpg\n",
            "Moved: Beige-6-_bmp.rf.30d201bae40307505d824b3decec1711.jpg\n",
            "Moved: Bone-7-_bmp.rf.7f9e5fc51e46aa0d012b3c57d8f8d3fa.jpg\n",
            "Moved: Yellow-8-_bmp.rf.2b64f79e96831332af17d4654525ceca.jpg\n",
            "Moved: Ping-16-_bmp.rf.f1e2fd3163e69bc0745c6c257e20ec89.jpg\n",
            "Moved: Triangular-21-_bmp.rf.7868376d398f50439b135eec0b8ad4a2.jpg\n",
            "Moved: SizeB-2-_bmp.rf.80967b25cf3d961bef78bd9ed5f9eb51.jpg\n",
            "Moved: BrokenBig-15-_bmp.rf.e265d4003a33ad5ced03862adb24d087.jpg\n",
            "Moved: Ping-14-_bmp.rf.8dc055ec5e5011da7c1b4d18bf386199.jpg\n",
            "Moved: Triangular-4-_bmp.rf.04c32eee560c392306830e6c8dbbb739.jpg\n",
            "Moved: Triangular-8-_bmp.rf.537360240fbbbc9009211d8ad732bcac.jpg\n",
            "Moved: Beige-7-_bmp.rf.9ccc44658b09189b661f99d6c1203dfe.jpg\n",
            "Moved: SizeC-14-_bmp.rf.3904812167919eb447ed106f40516f1b.jpg\n",
            "Moved: LightFeather-18-_bmp.rf.c84b66ac748b8a88925902b4337450cc.jpg\n",
            "Moved: MedFeather-9-_bmp.rf.78e72f3c780fb8904f5d1ad583edc751.jpg\n",
            "Moved: LightFeather-17-_bmp.rf.a60facb004e480d55ad2e51eafeb2e6d.jpg\n",
            "Moved: BrokenSmall-22-_bmp.rf.fe3951df62fa0a1754fa5d0b68572be3.jpg\n",
            "Moved: SizeA-7-_bmp.rf.450b9af5848a3d027260052459764f70.jpg\n",
            "Moved: WhiteBeige-15-_bmp.rf.033e2da8f980d2dd36d714bc7e7c6e08.jpg\n",
            "Moved: SizeC-5-_bmp.rf.c2bc497c1996bb560939c1bb709e2f9d.jpg\n",
            "Moved: MedFeather-16-_bmp.rf.3fe86b70c7cf7907d5a192493424f495.jpg\n",
            "Moved: SizeB-12-_bmp.rf.8b3c2e82d9ce0174261978dc7f0d6486.jpg\n",
            "Moved: Triangular-7-_bmp.rf.c5b15635b95fba587e54b1ca63d310ad.jpg\n",
            "Moved: BrokenSmall-20-_bmp.rf.119e06f1a64b0157a3e93115f4138589.jpg\n",
            "Moved: BrokenBig-5-_bmp.rf.c4b1f7e46712adc42a474acc574a519a.jpg\n",
            "Moved: SizeB-1-_bmp.rf.bbc00b67442b2269cff792c4a5784395.jpg\n",
            "Moved: BrokenSmall-34-_bmp.rf.9c78870f54b9320baea1f66c54868fb4.jpg\n",
            "Moved: LightFeather-8-_bmp.rf.6aadfc6b2210137c20441a02eb10eb35.jpg\n",
            "Moved: Ping-2-_bmp.rf.9bffc4f810c41969f5dcc0d9c5fb05f4.jpg\n",
            "Moved: White-18-_bmp.rf.fee91adb876f8b9a0cef8ad53d98d773.jpg\n",
            "Moved: BrokenBig-16-_bmp.rf.1c8d6483267f4751dc6b3ada8316a9f0.jpg\n",
            "Moved: Yellow-20-_bmp.rf.3f2f8d91fffe04f0641a29c6e0c18d9c.jpg\n",
            "Moved: SizeB-12-_bmp.rf.357c1e05f13b0a695e680d5ee32c3047.jpg\n",
            "Moved: Yellow-10-_bmp.rf.97a11dc2752a013a345a28ac9a16093d.jpg\n",
            "Moved: HeavyFeather-10-_bmp.rf.86588a67059d9e429919a2f3f113c455.jpg\n",
            "Moved: SizeC-6-_bmp.rf.f458237729d367aa78fdef123880edee.jpg\n",
            "Moved: MedFeather-7-_bmp.rf.c5b6ae671cf495420af2269ad4647aaf.jpg\n",
            "Moved: SizeC-13-_bmp.rf.a8a9e23ae3c4bbada4b14baa8fe8b14e.jpg\n",
            "Moved: HeavyFeather-7-_bmp.rf.a121f4a5a702cc85087ad5cca99b7d47.jpg\n",
            "Moved: Triangular-5-_bmp.rf.0a7712e8f3d3b364ecac9eed7899aa09.jpg\n",
            "Moved: White-1-_bmp.rf.87b863c547bfd34c15214815f7972924.jpg\n",
            "Moved: Triangular-20-_bmp.rf.496d059b30e8d51f8f38186455125c33.jpg\n",
            "Moved: MedFeather-20-_bmp.rf.946351f1097c7266e15acfe9bd469116.jpg\n",
            "Moved: White-6-_bmp.rf.28516d10d04b9450d4a13ee206358dde.jpg\n",
            "Moved: SizeC-17-_bmp.rf.3efb9755db5af9d93c997fd93c09ba91.jpg\n",
            "Moved: BrokenSmall-14-_bmp.rf.a0d3a01c518df77e5babfb723b31d4e3.jpg\n",
            "Moved: Bone-10-_bmp.rf.59d5b8fa5b728b762ecd55a89a330721.jpg\n",
            "Moved: Yellow-6-_bmp.rf.a31efab61e76083336d320c15b785bc8.jpg\n",
            "Moved: Triangular-23-_bmp.rf.93063b713f94717f65fffb9c78ed823a.jpg\n",
            "Moved: Grey-24-_bmp.rf.5769baf143ac5f1898789a3c4d9083c8.jpg\n",
            "Moved: SizeB-5-_bmp.rf.c97f58189a4ac17d31b30a46331964cf.jpg\n",
            "Moved: Grey-24-_bmp.rf.acd9c7aa48256ba52c32f2cd27c795b1.jpg\n",
            "Moved: Triangular-8-_bmp.rf.d27771587e1b004ebac396015cd2d361.jpg\n",
            "Moved: Ping-3-_bmp.rf.8ef4f44a941fe3fb665791f1ae0882c6.jpg\n",
            "Moved: Bone-12-_bmp.rf.995c31f2452b6a357221837644943e7a.jpg\n",
            "Moved: ThickFlesh-13-_bmp.rf.14e2b9da6f1ac54873373f7a7ac34e3c.jpg\n",
            "Moved: Beige-10-_bmp.rf.1db1c098494d3bdbbc52c4c8a7c3a277.jpg\n",
            "Moved: BrokenBig-8-_bmp.rf.4d0acd16981ef8c9af60e3d11325384b.jpg\n",
            "Moved: SizeC-12-_bmp.rf.c06e039113360e14fa9ab57271b86f90.jpg\n",
            "Moved: Yellow-15-_bmp.rf.3bdd9ad7ad8b891e6cbc8aeb6b3559d2.jpg\n",
            "Moved: MedFeather-6-_bmp.rf.71d7a2baad09423e8e4600d7ee919567.jpg\n",
            "Moved: Beige-7-_bmp.rf.ca52c6bc86aff7bcfcaccf762b079b37.jpg\n",
            "Moved: SizeC-6-_bmp.rf.b9487ad23ca4f24d66a52879749c9723.jpg\n",
            "Moved: ThickFlesh-6-_bmp.rf.ecf7c496f53cdffd5fd715891fb9f5fa.jpg\n",
            "Moved: ThickFlesh-7-_bmp.rf.f7284bd3737ae646c16d366c42d901f1.jpg\n",
            "Moved: Bone-9-_bmp.rf.cc47e90c8138d914cc882ad4b96e8dad.jpg\n",
            "Moved: BrokenBig-19-_bmp.rf.f056990ba4675d9d3430097ec8a82cdd.jpg\n",
            "Moved: Ping-10-_bmp.rf.83bd9192155c3a9bc0841b876866cf77.jpg\n",
            "Moved: Yellow-5-_bmp.rf.729b90b298c4edb206e276f9559aa201.jpg\n",
            "Moved: Yellow-15-_bmp.rf.a6152bb1d9b093fc07ef0a77c2771f6e.jpg\n",
            "Moved: BrokenSmall-71-_bmp.rf.c9ac7676a4ee5b1a7e1a042a65bd23e0.jpg\n",
            "Moved: MedFeather-7-_bmp.rf.727f7b52f8ff1e2fd67054300437b8d0.jpg\n",
            "Moved: BrokenBig-2-_bmp.rf.ba401d75a052926a924117ef83f04bed.jpg\n",
            "Moved: Beige-9-_bmp.rf.a18510b73d676c4e54d4c0072a84e81b.jpg\n",
            "Moved: MedFeather-5-_bmp.rf.5da31651fb95acaa5f8054d423f520b9.jpg\n",
            "Moved: LightFeather-9-_bmp.rf.fa0b01260993ce08a0e3e353259c1280.jpg\n",
            "Moved: Mossy-14-_bmp.rf.0a82ee171fca14e72119f43505be014a.jpg\n",
            "Moved: Beige-2-_bmp.rf.65e48d8cdb31455d5ee2bbef095787d6.jpg\n",
            "Moved: WhiteBeige-14-_bmp.rf.4f42e8e078ebb055a7feb8e3a12f4e3f.jpg\n",
            "Moved: Triangular-22-_bmp.rf.de4052bbe0e6bd3409d3aefd424b12ca.jpg\n",
            "Moved: Triangular-11-_bmp.rf.bd16d8a9857a89c9fda637ec28513744.jpg\n",
            "Moved: Bone-1-_bmp.rf.50b72a9dcd7ac6b26bb1e7a7ebc93592.jpg\n",
            "Moved: SizeB-10-_bmp.rf.559f7a336e3b78816ef9e4fa290caf04.jpg\n",
            "Moved: SizeC-8-_bmp.rf.c3f2b08024f40fce95b6b00391799e48.jpg\n",
            "Moved: BrokenSmall-18-_bmp.rf.cc88495fe97ba7511f3700d99a48ad90.jpg\n",
            "Moved: WhiteBeige-9-_bmp.rf.bfab449a7c50385fe74c4f732323a4ee.jpg\n",
            "Moved: BrokenBig-19-_bmp.rf.43a8ce83fcbe91b1cc1d9c783b09c7f9.jpg\n",
            "Moved: SizeA-6-_bmp.rf.a6c17881f2470b6447fc678d2380ed84.jpg\n",
            "Moved: White-6-_bmp.rf.61aed7e08483b4e64e9f567640ce91d5.jpg\n",
            "Moved: WhiteBeige-10-_bmp.rf.8611e8ba2319c50bb55ec8764e4dd4a2.jpg\n",
            "Moved: WhiteBeige-5-_bmp.rf.56f7b40b93cee11a05be06670a07c40f.jpg\n",
            "Moved: Bone-11-_bmp.rf.d3e66d366d4adcf5b6b1505519ebf212.jpg\n",
            "Moved: SizeA-4-_bmp.rf.6e171d39db1ac5409793eeadaa21895c.jpg\n",
            "Moved: WhiteBeige-7-_bmp.rf.919be83f62dd8232fbf8a6a09b892d2b.jpg\n",
            "Moved: ThickFlesh-4-_bmp.rf.3c30ac520c9eb8a6d55eba25fcf309eb.jpg\n",
            "Moved: MedFeather-5-_bmp.rf.3c9b38ff94b32433d525f5746ca5cd5d.jpg\n",
            "Moved: Yellow-11-_bmp.rf.06a3168781624fb9e604365191519cd7.jpg\n",
            "Moved: SizeA-14-_bmp.rf.8460bdc861659f0d3b1bf6a8093c7fa7.jpg\n",
            "Moved: Ping-15-_bmp.rf.14ab0b2d61dc530fea77236a9eb15111.jpg\n",
            "Moved: Yellow-1-_bmp.rf.2f2a332aaa728efcb69d721b94262b05.jpg\n",
            "Moved: Grey-9-_bmp.rf.734fa9b661a748ea3fdb2f2ea15fd6e0.jpg\n",
            "Moved: ThickFlesh-15-_bmp.rf.cfbad84f629a3390c70e284bff08bed7.jpg\n",
            "Moved: WhiteBeige-3-_bmp.rf.50e58edf97adc7ce6bed8243611a6748.jpg\n",
            "Moved: Mossy-13-_bmp.rf.f15d445872d0389a4ce09a8d3e4fda34.jpg\n",
            "Moved: SizeB-14-_bmp.rf.4ce62d9b2da145a11102252dd20e9487.jpg\n",
            "Moved: BrokenSmall-32-_bmp.rf.c96dea9d8cb6ddb119e7be69ef5059cb.jpg\n",
            "Moved: Mossy-7-_bmp.rf.00f9d1dfefd26beffe0b96b7246f038c.jpg\n",
            "Moved: Grey-9-_bmp.rf.800887c2aac9feaf9cf277f756f40f71.jpg\n",
            "Moved: Beige-2-_bmp.rf.f634a01afb7df0a515dc29de27c12bf1.jpg\n",
            "Moved: SizeA-7-_bmp.rf.84605326535888e4d968ffb838760881.jpg\n",
            "Moved: BrokenBig-2-_bmp.rf.44d795b20806819266af1d3956f63a32.jpg\n",
            "Moved: LightFeather-14-_bmp.rf.4adf3f8af4f7f269cb9122bcf6ee9494.jpg\n",
            "Moved: SizeA-3-_bmp.rf.15cb7eac9f85cc277495c01a38860b09.jpg\n",
            "Moved: BrokenBig-3-_bmp.rf.505ee61ffbec7e14741dc4149fb2d1a7.jpg\n",
            "Moved: WhiteBeige-3-_bmp.rf.23ac2458f33f2823b8118252e6aa4627.jpg\n",
            "Moved: Ping-12-_bmp.rf.3293fdfefb42dab0caf51693d3558b7b.jpg\n",
            "Moved: DoubleColour-3-_bmp.rf.47b0d126e26a895cf5466d980b2d8f84.jpg\n",
            "Moved: ThickFlesh-15-_bmp.rf.d6764ccc53875628b0e816e32c034d59.jpg\n",
            "Moved: ThickFlesh-14-_bmp.rf.be18231d9a5760026627a1d1ffb080b4.jpg\n",
            "Moved: BrokenSmall-26-_bmp.rf.3f6a3bdc5399453092132194aa1b5b2e.jpg\n",
            "Moved: MedFeather-4-_bmp.rf.fb35d0ad676ffe883049f591e1015ffe.jpg\n",
            "Moved: Beige-10-_bmp.rf.8c5beb84d4a76384c36d1a279059c1b9.jpg\n",
            "Moved: Bone-2-_bmp.rf.ad5d48cc10c8eb5d0a2563e4c6c96c5f.jpg\n",
            "Moved: Ping-16-_bmp.rf.2af772ce913f5cba58321929468fa21f.jpg\n",
            "Moved: ThickFlesh-4-_bmp.rf.fddbd81c72491f5740efd3b2bbcc40ff.jpg\n",
            "Moved: LightFeather-5-_bmp.rf.94cd610fb900a6bbea555d04c4584642.jpg\n",
            "Moved: Triangular-6-_bmp.rf.63ac94cb02e4b38814452b0cb0b127e9.jpg\n",
            "Moved: Yellow-10-_bmp.rf.41c0b571979a5fcd10082aaa70e496de.jpg\n",
            "Moved: Yellow-3-_bmp.rf.12c3d17a504af3920addb79bc81452ba.jpg\n",
            "Moved: Ping-10-_bmp.rf.f8cb4bb243ab60880d61ed7fa31e74c0.jpg\n",
            "Moved: White-6-_bmp.rf.aa191690c546342e3cf8bfc8ddc26975.jpg\n",
            "Moved: DoubleColour-3-_bmp.rf.7efff22b51d2bac457b4e87dbdd7bfc5.jpg\n",
            "Moved: White-16-_bmp.rf.46500d6802424138a56df3de8d10f3b8.jpg\n",
            "Moved: SizeA-6-_bmp.rf.4ed583136c778105e69a9e92bcd81b51.jpg\n",
            "Moved: Triangular-7-_bmp.rf.384ca0cbd408087d921305f4a3d467c6.jpg\n",
            "Moved: Mossy-9-_bmp.rf.c072433e681279ededeb3a3349fa3906.jpg\n",
            "Moved: HeavyFeather-11-_bmp.rf.99e6a9b79b1d8104bf124e3fef8a9404.jpg\n",
            "Moved: Strips-8-_bmp.rf.85f23fb5a81c2ec8d4c92aadfff31436.jpg\n",
            "Moved: HeavyFeather-6-_bmp.rf.7db2bde374b4a811fbc309a3e51aca28.jpg\n",
            "Moved: Mossy-2-_bmp.rf.9ff03b0dd244531d612f83818f685436.jpg\n",
            "Moved: ThickFlesh-8-_bmp.rf.505d5aeda9bdec4e1fc7f7cebdef631c.jpg\n",
            "Moved: Strips-17-_bmp.rf.da831285b7e00e6b06e76426fe85ee30.jpg\n",
            "Moved: Beige-14-_bmp.rf.4918e39b4ecad96c96f44dcb353229e9.jpg\n",
            "Moved: Grey-7-_bmp.rf.83ad1a68ee1930f7ec879e8251f321b9.jpg\n",
            "Moved: Triangular-15-_bmp.rf.980440daefe1da2539a70dc11cfe634b.jpg\n",
            "Moved: Beige-16-_bmp.rf.0560044c2c2223e6cbd356eb9a47cddf.jpg\n",
            "Moved: Bone-5-_bmp.rf.21de65f7270a3a0d919ac05b17b7e2e0.jpg\n",
            "Moved: ThickFlesh-2-_bmp.rf.554acbab39b29ef55e269a20e42b6949.jpg\n",
            "Moved: HeavyFeather-13-_bmp.rf.10ba8eac239dbc5bfc523d02a12724bf.jpg\n",
            "Moved: Grey-3-_bmp.rf.57a4bfb5ddfcc72c7f1a48ee0907c6a5.jpg\n",
            "Moved: Shredded-2-_bmp.rf.6e61e9c544524b9cef0c618f78b5fb77.jpg\n",
            "Moved: DaYuenJiao-13-_bmp.rf.6aeed37667841a4b4c0bd367b9781944.jpg\n",
            "Moved: HeavyFeather-18-_bmp.rf.07e65a98a777e3de21a4d610611f31ce.jpg\n",
            "Moved: Strips-2-_bmp.rf.49191369d669495dea1806348ff99202.jpg\n",
            "Moved: BrokenBig-6-_bmp.rf.285ff46af91e6d5b948a35fb872d5314.jpg\n",
            "Moved: HeavyFeather-4-_bmp.rf.bd79a740ccdd48f2afb3812ebba9a5a6.jpg\n",
            "Moved: Yellow-2-_bmp.rf.e64e6029d06ba84ab840fe4848988ea3.jpg\n",
            "Moved: Triangular-10-_bmp.rf.1d94064fce46d64bf1547d8096ae5eff.jpg\n",
            "Moved: Ping-4-_bmp.rf.c4709acff4557b57c00eb05ec53bf2f1.jpg\n",
            "Moved: Grey-11-_bmp.rf.ccdbd54d2c74bf7169235efdd81ca42d.jpg\n",
            "Moved: Grey-22-_bmp.rf.5e02bbed48ad61432256ee293d438aa4.jpg\n",
            "Moved: Mossy-8-_bmp.rf.136d342498facb5acedeadcdd4c4a7b1.jpg\n",
            "Moved: ThickFlesh-16-_bmp.rf.a808ec232a5a655a783b2ba28f13321b.jpg\n",
            "Moved: ThickFlesh-18-_bmp.rf.1414c3bdabdef91e26d15c62b34aa67e.jpg\n",
            "Moved: Triangular-14-_bmp.rf.a2b09cc93c4fdeacc3f7efc2522224f1.jpg\n",
            "Moved: Strips-3-_bmp.rf.dc680a8a699a937ec6664160a81c4e00.jpg\n",
            "Moved: White-10-_bmp.rf.e913dff74747a0855d9af021369c5529.jpg\n",
            "Moved: BrokenSmall-42-_bmp.rf.42496c0cdabe0a9ba3d32d7ea4c076e1.jpg\n",
            "Moved: SizeA-5-_bmp.rf.c35a99ba3faa1b6b352b9b85de799fe3.jpg\n",
            "Moved: Grey-15-_bmp.rf.a77290b97e2611d3c34080bfadfd8910.jpg\n",
            "Moved: HeavyFeather-16-_bmp.rf.ba77cce185f71aae30662be29dc4c8cb.jpg\n",
            "Moved: White-5-_bmp.rf.97b090d04802ad0bc9c8a0a139f52b40.jpg\n",
            "Moved: Mossy-12-_bmp.rf.eead3c8dec6d4c46cdc94e5b92a5efed.jpg\n",
            "Moved: BrokenSmall-19-_bmp.rf.233e1c185f4925b4a71db223eaa357eb.jpg\n",
            "Moved: Strips-11-_bmp.rf.de1dc59717d6f40679961ad3f2680061.jpg\n",
            "Moved: LightFeather-4-_bmp.rf.7e8141e30c1690e5ebd1704a85b16b7f.jpg\n",
            "Moved: Strips-13-_bmp.rf.a726a73619e12b1d78cff4e500112939.jpg\n",
            "Moved: Grey-1-_bmp.rf.27ac41bb9f80e23848127b670fb2d290.jpg\n",
            "Moved: White-14-_bmp.rf.4a0a25eafe275f988cbc3ee1def22f9d.jpg\n",
            "Moved: Strips-14-_bmp.rf.1191cf23e945edc467be585e9164654a.jpg\n",
            "Moved: Grey-21-_bmp.rf.931f174a26668353712420a9de7cd84d.jpg\n",
            "Moved: Strips-16-_bmp.rf.ab15cbccc97b5aa6292f9e7e76298d13.jpg\n",
            "Moved: Grey-19-_bmp.rf.6c545752fbd482dfee33cc915a07e2b1.jpg\n",
            "Moved: ThickFlesh-10-_bmp.rf.c7ab42c174f017abd2a4a2ebc65874a5.jpg\n",
            "Moved: Strips-5-_bmp.rf.87b78dc23918ecc47e111b4f327db6df.jpg\n",
            "Moved: Grey-18-_bmp.rf.819e4fd364aa0b260f2c89654d38acb7.jpg\n",
            "Moved: MedFeather-2-_bmp.rf.bd9e62952ec4ae66247bb5a71f51babf.jpg\n",
            "Moved: Grey-8-_bmp.rf.841872152b81da9cc925409a8736d112.jpg\n",
            "Moved: Mossy-10-_bmp.rf.260ec78ea9da850180b71bc566b48701.jpg\n",
            "Moved: SizeC-10-_bmp.rf.109df476d51793c4fbf52612a02f8ff9.jpg\n",
            "Moved: Grey-4-_bmp.rf.15a36aaa256dda04a36747307ca1b146.jpg\n",
            "Moved: Mossy-16-_bmp.rf.75071f5d5f684f18b96667575c6aef55.jpg\n",
            "Moved: Grey-6-_bmp.rf.2b1eaa9ba04da6bb969f8de39980919d.jpg\n",
            "Moved: DoubleColour-9-_bmp.rf.72baf0a42f60be6ff397be6874a5ea10.jpg\n",
            "Moved: Ping-1-_bmp.rf.782a51d2a8502064d145c5b2f5de7f76.jpg\n",
            "Moved: DoubleColour-11-_bmp.rf.852effa251ac0d8c7e5306deaa620a4f.jpg\n",
            "Moved: Grey-14-_bmp.rf.d0aa59ef1b2082c27b62a8f97de56287.jpg\n",
            "Moved: HeavyFeather-8-_bmp.rf.4f2abb8f5d67692d42e2536d7dbe472e.jpg\n",
            "Moved: Triangular-1-_bmp.rf.2f7357fbf1812027bb941533a6eb55b2.jpg\n",
            "Moved: Grey-12-_bmp.rf.4e3dd02d92856a3507d5383fafc86c08.jpg\n",
            "Moved: Strips-7-_bmp.rf.c63f4e0e8a8c58b2ac3940426fd475c1.jpg\n",
            "Moved: Mossy-5-_bmp.rf.dbf610df3a4ca9702cda2d805037d589.jpg\n",
            "Moved: MedFeather-18-_bmp.rf.1a6c9eb65d7bfdf6b7035b30792eaa90.jpg\n",
            "Moved: Strips-4-_bmp.rf.92e90f6d7f40777267cac5c278b350ce.jpg\n",
            "Moved: DoubleColour-1-_bmp.rf.2473f2434cc90e37f595762e30f1e602.jpg\n",
            "Moved: Beige-15-_bmp.rf.7ed33d4dc16294dd265aa14d3ac323e7.jpg\n",
            "Moved: ThickFlesh-12-_bmp.rf.7ede9ed0bd29b0d18404acec1d91b2b4.jpg\n",
            "Moved: ThickFlesh-9-_bmp.rf.668b3805daefda8e4a2b2524e07dceba.jpg\n",
            "Moved: Grey-23-_bmp.rf.eb7b2a496c02c5b1d04a68e6b5ee6675.jpg\n",
            "Moved: HeavyFeather-14-_bmp.rf.f900e4f016aa98994a06086594140f43.jpg\n",
            "Moved: LightFeather-6-_bmp.rf.d83e1f394386808004ff9718fa5c7365.jpg\n",
            "Moved: HeavyFeather-15-_bmp.rf.5a89a8cfc867229853555c63aa8cd5f4.jpg\n",
            "Moved: Mossy-4-_bmp.rf.57bc698b66045999999985f61ec12382.jpg\n",
            "Moved: Triangular-13-_bmp.rf.ea74a323106892fb9f16caeaeb6cbb17.jpg\n",
            "Moved: HeavyFeather-12-_bmp.rf.1c9ef81e6c2914dba9f4bf708b66beaa.jpg\n",
            "Moved: Mossy-15-_bmp.rf.769a12f27a521a138e5b1f671faf5ddc.jpg\n",
            "Moved: BrokenSmall-47-_bmp.rf.e2445454ef248b7e1d0a15c59b079ce3.jpg\n",
            "Moved: Grey-2-_bmp.rf.613d5f3688e8e5f6e17e48fe57f963b5.jpg\n",
            "Moved: MedFeather-8-_bmp.rf.33133482235f44fd3c2eb5ba13d6e324.jpg\n",
            "Moved: Grey-5-_bmp.rf.1a8c5171d76eeb25437d2492da50a471.jpg\n",
            "Moved: DoubleColour-5-_bmp.rf.1f84d56e4124ff27e5691d0b32b8b850.jpg\n",
            "Moved: HeavyFeather-17-_bmp.rf.cfe425bfcd8dcc4619c3db08fb59b81e.jpg\n",
            "Moved: Yellow-9-_bmp.rf.c60708a40e29488dcfe4a89f970c125e.jpg\n",
            "Moved: Grey-20-_bmp.rf.755da4ee65d32e7135760334b63f5fd2.jpg\n",
            "Moved: HeavyFeather-2-_bmp.rf.142f1eb0cbd7ea0ca8b22a5f2db3d2a1.jpg\n",
            "Moved: Strips-15-_bmp.rf.48898f65ed64a1d214523a6ca39f9e00.jpg\n",
            "Moved: Strips-1-_bmp.rf.8a93b9ad3117a06ca7daa5b9c80c787f.jpg\n",
            "Moved: BrokenSmall-10-_bmp.rf.1224908977b6880abc2fd1d97cc59bc5.jpg\n",
            "Moved: BrokenSmall-23-_bmp.rf.b001115730b81d12c31a85031d4a5bd7.jpg\n",
            "Moved: BrokenSmall-46-_bmp.rf.d04df1397677d940ce9889949ae80ebe.jpg\n",
            "Moved: DaYuenJiao-14-_bmp.rf.5d96d053d9466787cbf65fadedcd1f4f.jpg\n",
            "Moved: BrokenSmall-74-_bmp.rf.e00f2c1c564c25bb06cfa486842bd6a0.jpg\n",
            "Moved: BrokenSmall-9-_bmp.rf.040ff164f83bf8e84f07ef1755240869.jpg\n",
            "Moved: BrokenSmall-16-_bmp.rf.8c4f96e867b599c796912451909517e0.jpg\n",
            "Moved: DoubleColour-8-_bmp.rf.4d6789a3591957c392396427cb9f56cd.jpg\n",
            "Moved: BrokenSmall-33-_bmp.rf.9b2588c367251b1a7782bcfe0fde93f2.jpg\n",
            "Moved: DoubleColour-10-_bmp.rf.6737fbd6808f7fdef361548a9c4021c9.jpg\n",
            "Moved: BrokenSmall-61-_bmp.rf.08ed635b42a3a5c50dac69f607b81721.jpg\n",
            "Moved: BrokenBig-14-_bmp.rf.4bf0500aaf849ab65a5fa65c47565bf5.jpg\n",
            "Moved: SizeB-4-_bmp.rf.93a855dc9e25ab78b0ef7e915c2e09b9.jpg\n",
            "Moved: BrokenSmall-40-_bmp.rf.9291d5b6db619440864f7074f082709f.jpg\n",
            "Moved: BrokenSmall-70-_bmp.rf.fb38d0f8915eff4926cc7d93fcbde095.jpg\n",
            "Moved: BrokenSmall-8-_bmp.rf.48d5a47abd9288a981ccbd9b71553f35.jpg\n",
            "Moved: BrokenSmall-57-_bmp.rf.d85595c7b490ba76572606438d360297.jpg\n",
            "Moved: BrokenSmall-1-_bmp.rf.ab4bda0dc421e080d1e351acae64fc3c.jpg\n",
            "Moved: DaYuenJiao-9-_bmp.rf.929cc84aa75d97c94f5dcbdf6c875bbb.jpg\n",
            "Moved: LightFeather-12-_bmp.rf.2da7b0ce8359c9f79edca76f40a54182.jpg\n",
            "Moved: Grey-10-_bmp.rf.4a8556132f380755482146c6d83e7f1c.jpg\n",
            "Moved: BrokenSmall-69-_bmp.rf.7c6fa49695293faa202be8c73ef7327b.jpg\n",
            "Moved: Yellow-14-_bmp.rf.736f07aab520a0daa093d891e6e6ab81.jpg\n",
            "Moved: BrokenSmall-51-_bmp.rf.a07934d8e8c150b6440a196432381d80.jpg\n",
            "Moved: LightFeather-11-_bmp.rf.332e4d814ba9b935b183c9236726f27f.jpg\n",
            "Moved: BrokenSmall-50-_bmp.rf.738a81c3933609ca5f7b5d7f01d35311.jpg\n",
            "Moved: DaYuenJiao-18-_bmp.rf.e17952b1d57bc94d1361746e251eb893.jpg\n",
            "Moved: BrokenSmall-59-_bmp.rf.5bb2ed6cd5b31c9da4ed413e0804e0d9.jpg\n",
            "Moved: BrokenSmall-68-_bmp.rf.3c2e8d6c3d05a3c8a3feacf9c1e634c4.jpg\n",
            "Moved: BrokenSmall-60-_bmp.rf.3513e6eb6ff678be64ad73ad827ba2f6.jpg\n",
            "Moved: BrokenSmall-73-_bmp.rf.bcab3d955ff9081ec859534e2031c841.jpg\n",
            "Moved: BrokenSmall-5-_bmp.rf.21346e523146ea8d9f02d035cafa38dd.jpg\n",
            "Moved: DoubleColour-6-_bmp.rf.487cf2aed5ea616e1b2f99d6bf067819.jpg\n",
            "Moved: BrokenSmall-52-_bmp.rf.e547fdf252e04beee4034e17571be2db.jpg\n",
            "Moved: BrokenSmall-7-_bmp.rf.8d79f140151e4948eb6baf1a79a94398.jpg\n",
            "Moved: DaYuenJiao-3-_bmp.rf.89848a6382edcc41d6697139b1a5b0b3.jpg\n",
            "Moved: BrokenSmall-3-_bmp.rf.1935744bb94aa98f8c32601e5add2d21.jpg\n",
            "Moved: BrokenSmall-54-_bmp.rf.407e71f749e51102d5e42ab42ad2abc2.jpg\n",
            "Moved: Ping-7-_bmp.rf.e63ae3bdbff078fcb7755d3d3b78431f.jpg\n",
            "Moved: BrokenSmall-63-_bmp.rf.a312ae826539903c0da3460be3d64c9c.jpg\n",
            "Moved: BrokenSmall-76-_bmp.rf.157d35d2cb106a2a54fd8ab1d023eb48.jpg\n",
            "Moved: MedFeather-10-_bmp.rf.af394eef82480a42c33a195c5178c37e.jpg\n",
            "Moved: BrokenSmall-75-_bmp.rf.2f62b9ae9380d9feabdbcbc80a140719.jpg\n",
            "Moved: DoubleColour-15-_bmp.rf.c035b615de7dc53db0703737012bf7e4.jpg\n",
            "Moved: DoubleColour-4-_bmp.rf.5eadf0f2c409b11498d207bc833f6fd6.jpg\n",
            "Moved: DaYuenJiao-8-_bmp.rf.0f0de616c36ad536bb339954da8c3cb7.jpg\n",
            "Moved: WhiteBeige-12-_bmp.rf.9fa26d21ca8a4984fa9b841ecab59d9c.jpg\n",
            "Moved: DaYuenJiao-2-_bmp.rf.fb4c2afd10f2687613ca14b0efc4a8a1.jpg\n",
            "Moved: MedFeather-12-_bmp.rf.4a7cc78d3c3f1877d5d5c1c898e1ecd0.jpg\n",
            "Moved: WhiteBeige-1-_bmp.rf.d1807d471eb6f43f562207763a110e05.jpg\n",
            "Moved: BrokenSmall-53-_bmp.rf.34e78866b25795c9f5f058d51806ad95.jpg\n",
            "Moved: LightFeather-1-_bmp.rf.702694255ff895ee93304bbb5b9cb69b.jpg\n",
            "Moved: DaYuenJiao-15-_bmp.rf.e41ec60432ea1cad8b47a3aac00c64fd.jpg\n",
            "Moved: BrokenSmall-41-_bmp.rf.c4ea94498fe936421ff18bd9d162dfe3.jpg\n",
            "Moved: DaYuenJiao-5-_bmp.rf.711b4f2049ba5c46dfdd8385afbe9131.jpg\n",
            "Moved: BrokenSmall-15-_bmp.rf.b20a51f7f8fcca7cb3cfdb0ab16611cb.jpg\n",
            "Moved: DaYuenJiao-11-_bmp.rf.679cb8e19ab4e04e0231cba6215479de.jpg\n",
            "Moved: DaYuenJiao-10-_bmp.rf.3d581b889b2c2c5cb5b848a57fdedfcf.jpg\n",
            "Moved: BrokenSmall-67-_bmp.rf.b3efa94298b683173d4666b6e792f178.jpg\n",
            "Moved: BrokenSmall-12-_bmp.rf.33120b12fd0ad878c017340b4270f16d.jpg\n",
            "Moved: BrokenBig-4-_bmp.rf.13f0c976f7503a2b3f2b3c68ae6e8c0f.jpg\n",
            "Moved: DaYuenJiao-4-_bmp.rf.3811c4c630c2125c32b142d1f7bc4d0f.jpg\n",
            "Moved: BrokenSmall-44-_bmp.rf.8a12978a79ccad5dbd9530e2c7938c62.jpg\n",
            "Moved: BrokenBig-18-_bmp.rf.651c442730a0c71edafebf1868f5ab06.jpg\n",
            "Moved: LightFeather-13-_bmp.rf.c39e96d15542e6f06183c7c13674ff9b.jpg\n",
            "Moved: DaYuenJiao-16-_bmp.rf.76876a8f6f27f571c25a94d70d09f281.jpg\n",
            "Moved: DaYuenJiao-12-_bmp.rf.c2bc20accf02bee5e4a008fa33b0abde.jpg\n",
            "Moved: DaYuenJiao-6-_bmp.rf.2782b420d7f2233e0f043c48bba1a686.jpg\n",
            "Moved: BrokenSmall-65-_bmp.rf.9c56583afaf593a0d512548d391fd7f1.jpg\n",
            "Moved: DoubleColour-14-_bmp.rf.22b2c662b003895b5ba2b359218dc9f2.jpg\n",
            "Moved: BrokenSmall-4-_bmp.rf.2aee3c5587b6fa4eb7f861e8c2dda0fd.jpg\n",
            "Moved: BrokenSmall-56-_bmp.rf.7cd6234f27024f8559a03b0fb3c4c3e7.jpg\n",
            "Moved: BrokenSmall-62-_bmp.rf.cfe2df6eb85a28e0244f979f71d508c2.jpg\n",
            "Moved: BrokenSmall-43-_bmp.rf.5389138151fb6c8c798cbbff7546d7d6.jpg\n",
            "Moved: BrokenSmall-64-_bmp.rf.ca5c24075c5e328f83fde23b3b1999ef.jpg\n",
            "Moved: BrokenSmall-72-_bmp.rf.4dc02ec999175386231df32c21c4d130.jpg\n",
            "Moved: DaYuenJiao-1-_bmp.rf.c89a051146742c8f472220f51291117b.jpg\n",
            "Moved: BrokenSmall-58-_bmp.rf.dec8d5bab59b867672a127851733ed6d.jpg\n",
            "Moved: DoubleColour-12-_bmp.rf.2702830e629796218fdf08ef3f4b27f2.jpg\n",
            "Moved: BrokenSmall-49-_bmp.rf.ff5ea777b15fa2cf463e4659dd49bdac.jpg\n",
            "Moved: BrokenSmall-38-_bmp.rf.f9e5f55e960224b104010806789677d1.jpg\n",
            "Moved: BrokenSmall-35-_bmp.rf.69e8c4c4a0be6192503e862ee946a0a0.jpg\n",
            "Moved: BrokenSmall-39-_bmp.rf.c831e95af472c74100ab5d8aad34c531.jpg\n",
            "Moved: DoubleColour-7-_bmp.rf.064f0a934eac64b930017d0613a22dd7.jpg\n",
            "Moved: DoubleColour-13-_bmp.rf.4961b0e8b5e458f85a734062151db36e.jpg\n",
            "Moved: Beige-4-_bmp.rf.236ac3c150eb68adb1c1210c571b98ee.jpg\n",
            "Moved: BrokenSmall-36-_bmp.rf.258c4c8e4c03e721bc3d3c2de0afe346.jpg\n",
            "Moved: BrokenSmall-48-_bmp.rf.3ccdd4fa83af54b75dd31618b020c7c0.jpg\n",
            "Moved: BrokenSmall-45-_bmp.rf.9dd079cddb3c7aeb0bc04d4c35aa1b8d.jpg\n",
            "Moved: BrokenSmall-28-_bmp.rf.45ed9c7401450970bee299ddde16a764.jpg\n",
            "Moved: DaYuenJiao-17-_bmp.rf.100a6dbc3899ae283b66652bd274e25b.jpg\n",
            "Moved: WhiteBeige-6-_bmp.rf.d6117fbf124c0e711a75c0d7ee860360.jpg\n",
            "Moved: BrokenSmall-6-_bmp.rf.4a33c49e7891102f4c1337ebaabc3873.jpg\n",
            "Moved: BrokenSmall-55-_bmp.rf.266fa2608255bea039e699850197990f.jpg\n",
            "Moved: BrokenSmall-66-_bmp.rf.5af50e367735ed29d7e6e58450445354.jpg\n",
            "Moved: Ping-8-_bmp.rf.b1eb716fb5323dad269859667d717995.jpg\n"
          ]
        }
      ],
      "source": [
        "# move all the images into the workspace train, validate, and test directories\n",
        "import shutil\n",
        "\n",
        "source_folders = [paths['ROBOFLOW_TRAIN_PATH'], paths['ROBOFLOW_VALIDATE_PATH'], paths['ROBOFLOW_TEST_PATH']]\n",
        "destination_folders = [os.path.join(paths['IMAGE_PATH'], 'train'), os.path.join(paths['IMAGE_PATH'], 'valid'), os.path.join(paths['IMAGE_PATH'], 'test')]\n",
        "\n",
        "for i in range(len(source_folders)):\n",
        "  source_folder = source_folders[i]\n",
        "  destination_folder = destination_folders[i]\n",
        "  # fetch all files\n",
        "  for file_name in os.listdir(source_folder):\n",
        "      # construct full file path\n",
        "      source = os.path.join(source_folder, file_name)\n",
        "      destination = os.path.join(destination_folder, file_name)\n",
        "      # move only files\n",
        "      if os.path.isfile(source):\n",
        "          shutil.move(source, destination)\n",
        "          print('Moved:', file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VSk_vLqgk1Nj"
      },
      "outputs": [],
      "source": [
        "# remove the bird nest image directory\n",
        "!rm -r Bird-Nest-15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDVgk9NplDnZ",
        "outputId": "f7e982ee-a3d1-4445-94c5-5e6887b328c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Bird-Nest-15 to tfrecord:: 100%|██████████| 87751/87751 [00:02<00:00, 35922.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Bird-Nest-15 in tfrecord:: 100%|██████████| 11/11 [00:00<00:00, 18.26it/s]\n"
          ]
        }
      ],
      "source": [
        "# download the tfrecord files from roboflow\n",
        "rf = Roboflow(api_key=\"1woWfE1q4RoyHytXmktz\")\n",
        "project = rf.workspace(\"ebn\").project(\"bird-nest-exr6l\")\n",
        "dataset = project.version(15).download(\"tfrecord\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Siymrf2zrG8S"
      },
      "outputs": [],
      "source": [
        "# move the tfrecord files to the annotation path\n",
        "source_train = os.path.join(paths['ROBOFLOW_TRAIN_PATH'],'Bird-Nest.tfrecord')\n",
        "destination_train = os.path.join(paths['ANNOTATION_PATH'], 'train.record')\n",
        "\n",
        "source_test = os.path.join(paths['ROBOFLOW_TEST_PATH'],'Bird-Nest.tfrecord')\n",
        "destination_test = os.path.join(paths['ANNOTATION_PATH'], 'test.record')\n",
        "\n",
        "source_valid = os.path.join(paths['ROBOFLOW_VALIDATE_PATH'],'Bird-Nest.tfrecord')\n",
        "destination_valid = os.path.join(paths['ANNOTATION_PATH'], 'valid.record')\n",
        "\n",
        "os.rename(source_train, destination_train)\n",
        "os.rename(source_test, destination_test)\n",
        "os.rename(source_valid, destination_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "C9Xf_DuRTm_X"
      },
      "outputs": [],
      "source": [
        "# remove the bird nest image directory\n",
        "!rm -r Bird-Nest-15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT4QU7pLpfDE"
      },
      "source": [
        "# 4. Copy Model Config to Training Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cOjuTFbwpfDF"
      },
      "outputs": [],
      "source": [
        "if os.name =='posix':\n",
        "    !cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}\n",
        "if os.name == 'nt':\n",
        "    !copy {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga8gpNslpfDF"
      },
      "source": [
        "# 5. Update Config For Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Z9hRrO_ppfDF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.protos import pipeline_pb2\n",
        "from google.protobuf import text_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "c2A0mn4ipfDF"
      },
      "outputs": [],
      "source": [
        "# get current configurations\n",
        "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQA13-afpfDF"
      },
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9vK5lotDpfDF"
      },
      "outputs": [],
      "source": [
        "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:\n",
        "    proto_str = f.read()\n",
        "    text_format.Merge(proto_str, pipeline_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rP43Ph0JpfDG"
      },
      "outputs": [],
      "source": [
        "# updating the config file\n",
        "pipeline_config.model.ssd.num_classes = len(labels)\n",
        "pipeline_config.train_config.batch_size = 4\n",
        "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
        "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
        "pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n",
        "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
        "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
        "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oJvfgwWqpfDG"
      },
      "outputs": [],
      "source": [
        "# write out update to the config file\n",
        "config_text = text_format.MessageToString(pipeline_config)\n",
        "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:\n",
        "    f.write(config_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr3ON7xMpfDG"
      },
      "source": [
        "# 6. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "B-Y2UQmQpfDG"
      },
      "outputs": [],
      "source": [
        "# define the training script path\n",
        "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jMP2XDfQpfDH"
      },
      "outputs": [],
      "source": [
        "# define the train function\n",
        "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=5000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3ZsJR-qpfDH",
        "outputId": "327dc18e-1cd8-4a40-ba33-b65ddb47eeb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-28 08:54:03.908834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-10-28 08:54:08.947954: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1028 08:54:08.949138 139013598367744 mirrored_strategy.py:419] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 5000\n",
            "I1028 08:54:08.973775 139013598367744 config_util.py:552] Maybe overwriting train_steps: 5000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I1028 08:54:08.973968 139013598367744 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W1028 08:54:09.002936 139013598367744 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['Tensorflow/workspace/annotations/train.record']\n",
            "I1028 08:54:09.010723 139013598367744 dataset_builder.py:162] Reading unweighted datasets: ['Tensorflow/workspace/annotations/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['Tensorflow/workspace/annotations/train.record']\n",
            "I1028 08:54:09.010921 139013598367744 dataset_builder.py:79] Reading record datasets for input file: ['Tensorflow/workspace/annotations/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I1028 08:54:09.011016 139013598367744 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1028 08:54:09.011097 139013598367744 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W1028 08:54:09.018337 139013598367744 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1028 08:54:09.042680 139013598367744 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W1028 08:54:18.012179 139013598367744 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W1028 08:54:22.612146 139013598367744 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W1028 08:54:25.072489 139013598367744 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn(\n",
            "I1028 08:54:33.292272 139008388625984 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1028 08:54:45.507900 139008388625984 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "2023-10-28 08:54:48.753357: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20460000 exceeds 10% of free system memory.\n",
            "2023-10-28 08:54:49.191288: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20460000 exceeds 10% of free system memory.\n",
            "2023-10-28 08:54:49.202625: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20460000 exceeds 10% of free system memory.\n",
            "2023-10-28 08:54:49.225839: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20460000 exceeds 10% of free system memory.\n",
            "2023-10-28 08:54:49.237243: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20460000 exceeds 10% of free system memory.\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.412613 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.416509 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.417773 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.418826 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.423646 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.424826 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.426102 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.427174 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.433070 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1028 08:54:57.434214 139013598367744 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W1028 08:54:59.223095 139008586307136 deprecation.py:569] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "I1028 08:55:00.848714 139008586307136 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1028 08:55:09.004174 139008586307136 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1028 08:55:13.718142 139008586307136 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1028 08:55:19.419802 139008586307136 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "INFO:tensorflow:Step 100 per-step time 0.525s\n",
            "I1028 08:55:51.104030 139013598367744 model_lib_v2.py:705] Step 100 per-step time 0.525s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.5159603,\n",
            " 'Loss/localization_loss': 0.39715043,\n",
            " 'Loss/regularization_loss': 0.15183032,\n",
            " 'Loss/total_loss': 1.064941,\n",
            " 'learning_rate': 0.0319994}\n",
            "I1028 08:55:51.104441 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.5159603,\n",
            " 'Loss/localization_loss': 0.39715043,\n",
            " 'Loss/regularization_loss': 0.15183032,\n",
            " 'Loss/total_loss': 1.064941,\n",
            " 'learning_rate': 0.0319994}\n",
            "INFO:tensorflow:Step 200 per-step time 0.111s\n",
            "I1028 08:56:02.186657 139013598367744 model_lib_v2.py:705] Step 200 per-step time 0.111s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.352412,\n",
            " 'Loss/localization_loss': 0.3431188,\n",
            " 'Loss/regularization_loss': 0.15171798,\n",
            " 'Loss/total_loss': 0.84724873,\n",
            " 'learning_rate': 0.0373328}\n",
            "I1028 08:56:02.186962 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.352412,\n",
            " 'Loss/localization_loss': 0.3431188,\n",
            " 'Loss/regularization_loss': 0.15171798,\n",
            " 'Loss/total_loss': 0.84724873,\n",
            " 'learning_rate': 0.0373328}\n",
            "INFO:tensorflow:Step 300 per-step time 0.132s\n",
            "I1028 08:56:15.364815 139013598367744 model_lib_v2.py:705] Step 300 per-step time 0.132s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.3599109,\n",
            " 'Loss/localization_loss': 0.08323562,\n",
            " 'Loss/regularization_loss': 0.15143345,\n",
            " 'Loss/total_loss': 0.59458,\n",
            " 'learning_rate': 0.0426662}\n",
            "I1028 08:56:15.365096 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.3599109,\n",
            " 'Loss/localization_loss': 0.08323562,\n",
            " 'Loss/regularization_loss': 0.15143345,\n",
            " 'Loss/total_loss': 0.59458,\n",
            " 'learning_rate': 0.0426662}\n",
            "INFO:tensorflow:Step 400 per-step time 0.107s\n",
            "I1028 08:56:26.067081 139013598367744 model_lib_v2.py:705] Step 400 per-step time 0.107s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2622286,\n",
            " 'Loss/localization_loss': 0.08866113,\n",
            " 'Loss/regularization_loss': 0.15105383,\n",
            " 'Loss/total_loss': 0.5019436,\n",
            " 'learning_rate': 0.047999598}\n",
            "I1028 08:56:26.067503 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.2622286,\n",
            " 'Loss/localization_loss': 0.08866113,\n",
            " 'Loss/regularization_loss': 0.15105383,\n",
            " 'Loss/total_loss': 0.5019436,\n",
            " 'learning_rate': 0.047999598}\n",
            "INFO:tensorflow:Step 500 per-step time 0.132s\n",
            "I1028 08:56:39.264926 139013598367744 model_lib_v2.py:705] Step 500 per-step time 0.132s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.24746192,\n",
            " 'Loss/localization_loss': 0.0706891,\n",
            " 'Loss/regularization_loss': 0.15063542,\n",
            " 'Loss/total_loss': 0.46878642,\n",
            " 'learning_rate': 0.053333}\n",
            "I1028 08:56:39.265236 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.24746192,\n",
            " 'Loss/localization_loss': 0.0706891,\n",
            " 'Loss/regularization_loss': 0.15063542,\n",
            " 'Loss/total_loss': 0.46878642,\n",
            " 'learning_rate': 0.053333}\n",
            "INFO:tensorflow:Step 600 per-step time 0.110s\n",
            "I1028 08:56:50.328509 139013598367744 model_lib_v2.py:705] Step 600 per-step time 0.110s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.27266046,\n",
            " 'Loss/localization_loss': 0.03465907,\n",
            " 'Loss/regularization_loss': 0.15018143,\n",
            " 'Loss/total_loss': 0.45750093,\n",
            " 'learning_rate': 0.0586664}\n",
            "I1028 08:56:50.328875 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.27266046,\n",
            " 'Loss/localization_loss': 0.03465907,\n",
            " 'Loss/regularization_loss': 0.15018143,\n",
            " 'Loss/total_loss': 0.45750093,\n",
            " 'learning_rate': 0.0586664}\n",
            "INFO:tensorflow:Step 700 per-step time 0.132s\n",
            "I1028 08:57:03.504078 139013598367744 model_lib_v2.py:705] Step 700 per-step time 0.132s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.26249525,\n",
            " 'Loss/localization_loss': 0.039925918,\n",
            " 'Loss/regularization_loss': 0.149663,\n",
            " 'Loss/total_loss': 0.45208415,\n",
            " 'learning_rate': 0.0639998}\n",
            "I1028 08:57:03.504363 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.26249525,\n",
            " 'Loss/localization_loss': 0.039925918,\n",
            " 'Loss/regularization_loss': 0.149663,\n",
            " 'Loss/total_loss': 0.45208415,\n",
            " 'learning_rate': 0.0639998}\n",
            "INFO:tensorflow:Step 800 per-step time 0.138s\n",
            "I1028 08:57:17.351849 139013598367744 model_lib_v2.py:705] Step 800 per-step time 0.138s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.255498,\n",
            " 'Loss/localization_loss': 0.058869474,\n",
            " 'Loss/regularization_loss': 0.14910318,\n",
            " 'Loss/total_loss': 0.46347064,\n",
            " 'learning_rate': 0.069333196}\n",
            "I1028 08:57:17.352242 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.255498,\n",
            " 'Loss/localization_loss': 0.058869474,\n",
            " 'Loss/regularization_loss': 0.14910318,\n",
            " 'Loss/total_loss': 0.46347064,\n",
            " 'learning_rate': 0.069333196}\n",
            "INFO:tensorflow:Step 900 per-step time 0.122s\n",
            "I1028 08:57:29.559696 139013598367744 model_lib_v2.py:705] Step 900 per-step time 0.122s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.27895993,\n",
            " 'Loss/localization_loss': 0.041745607,\n",
            " 'Loss/regularization_loss': 0.14852604,\n",
            " 'Loss/total_loss': 0.46923158,\n",
            " 'learning_rate': 0.074666604}\n",
            "I1028 08:57:29.559982 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.27895993,\n",
            " 'Loss/localization_loss': 0.041745607,\n",
            " 'Loss/regularization_loss': 0.14852604,\n",
            " 'Loss/total_loss': 0.46923158,\n",
            " 'learning_rate': 0.074666604}\n",
            "INFO:tensorflow:Step 1000 per-step time 0.121s\n",
            "I1028 08:57:41.708956 139013598367744 model_lib_v2.py:705] Step 1000 per-step time 0.121s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.26484594,\n",
            " 'Loss/localization_loss': 0.02902106,\n",
            " 'Loss/regularization_loss': 0.14787568,\n",
            " 'Loss/total_loss': 0.44174266,\n",
            " 'learning_rate': 0.08}\n",
            "I1028 08:57:41.709322 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.26484594,\n",
            " 'Loss/localization_loss': 0.02902106,\n",
            " 'Loss/regularization_loss': 0.14787568,\n",
            " 'Loss/total_loss': 0.44174266,\n",
            " 'learning_rate': 0.08}\n",
            "INFO:tensorflow:Step 1100 per-step time 0.134s\n",
            "I1028 08:57:55.094233 139013598367744 model_lib_v2.py:705] Step 1100 per-step time 0.134s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.23668359,\n",
            " 'Loss/localization_loss': 0.05432448,\n",
            " 'Loss/regularization_loss': 0.14730363,\n",
            " 'Loss/total_loss': 0.4383117,\n",
            " 'learning_rate': 0.07999918}\n",
            "I1028 08:57:55.094624 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.23668359,\n",
            " 'Loss/localization_loss': 0.05432448,\n",
            " 'Loss/regularization_loss': 0.14730363,\n",
            " 'Loss/total_loss': 0.4383117,\n",
            " 'learning_rate': 0.07999918}\n",
            "INFO:tensorflow:Step 1200 per-step time 0.128s\n",
            "I1028 08:58:07.872617 139013598367744 model_lib_v2.py:705] Step 1200 per-step time 0.128s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.25482014,\n",
            " 'Loss/localization_loss': 0.027072357,\n",
            " 'Loss/regularization_loss': 0.1466952,\n",
            " 'Loss/total_loss': 0.4285877,\n",
            " 'learning_rate': 0.079996705}\n",
            "I1028 08:58:07.873045 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.25482014,\n",
            " 'Loss/localization_loss': 0.027072357,\n",
            " 'Loss/regularization_loss': 0.1466952,\n",
            " 'Loss/total_loss': 0.4285877,\n",
            " 'learning_rate': 0.079996705}\n",
            "INFO:tensorflow:Step 1300 per-step time 0.113s\n",
            "I1028 08:58:19.129582 139013598367744 model_lib_v2.py:705] Step 1300 per-step time 0.113s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.20273833,\n",
            " 'Loss/localization_loss': 0.018200511,\n",
            " 'Loss/regularization_loss': 0.14607668,\n",
            " 'Loss/total_loss': 0.36701554,\n",
            " 'learning_rate': 0.0799926}\n",
            "I1028 08:58:19.129875 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.20273833,\n",
            " 'Loss/localization_loss': 0.018200511,\n",
            " 'Loss/regularization_loss': 0.14607668,\n",
            " 'Loss/total_loss': 0.36701554,\n",
            " 'learning_rate': 0.0799926}\n",
            "INFO:tensorflow:Step 1400 per-step time 0.132s\n",
            "I1028 08:58:32.358723 139013598367744 model_lib_v2.py:705] Step 1400 per-step time 0.132s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.16684854,\n",
            " 'Loss/localization_loss': 0.07687092,\n",
            " 'Loss/regularization_loss': 0.14549308,\n",
            " 'Loss/total_loss': 0.38921255,\n",
            " 'learning_rate': 0.07998685}\n",
            "I1028 08:58:32.359091 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.16684854,\n",
            " 'Loss/localization_loss': 0.07687092,\n",
            " 'Loss/regularization_loss': 0.14549308,\n",
            " 'Loss/total_loss': 0.38921255,\n",
            " 'learning_rate': 0.07998685}\n",
            "INFO:tensorflow:Step 1500 per-step time 0.110s\n",
            "I1028 08:58:43.341642 139013598367744 model_lib_v2.py:705] Step 1500 per-step time 0.110s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.17186907,\n",
            " 'Loss/localization_loss': 0.022358075,\n",
            " 'Loss/regularization_loss': 0.14486289,\n",
            " 'Loss/total_loss': 0.33909005,\n",
            " 'learning_rate': 0.07997945}\n",
            "I1028 08:58:43.342060 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.17186907,\n",
            " 'Loss/localization_loss': 0.022358075,\n",
            " 'Loss/regularization_loss': 0.14486289,\n",
            " 'Loss/total_loss': 0.33909005,\n",
            " 'learning_rate': 0.07997945}\n",
            "INFO:tensorflow:Step 1600 per-step time 0.134s\n",
            "I1028 08:58:56.728801 139013598367744 model_lib_v2.py:705] Step 1600 per-step time 0.134s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.19064587,\n",
            " 'Loss/localization_loss': 0.018747997,\n",
            " 'Loss/regularization_loss': 0.14416775,\n",
            " 'Loss/total_loss': 0.35356164,\n",
            " 'learning_rate': 0.079970405}\n",
            "I1028 08:58:56.729085 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.19064587,\n",
            " 'Loss/localization_loss': 0.018747997,\n",
            " 'Loss/regularization_loss': 0.14416775,\n",
            " 'Loss/total_loss': 0.35356164,\n",
            " 'learning_rate': 0.079970405}\n",
            "INFO:tensorflow:Step 1700 per-step time 0.110s\n",
            "I1028 08:59:07.713684 139013598367744 model_lib_v2.py:705] Step 1700 per-step time 0.110s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.27702832,\n",
            " 'Loss/localization_loss': 0.022011986,\n",
            " 'Loss/regularization_loss': 0.14348622,\n",
            " 'Loss/total_loss': 0.44252652,\n",
            " 'learning_rate': 0.07995972}\n",
            "I1028 08:59:07.714055 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.27702832,\n",
            " 'Loss/localization_loss': 0.022011986,\n",
            " 'Loss/regularization_loss': 0.14348622,\n",
            " 'Loss/total_loss': 0.44252652,\n",
            " 'learning_rate': 0.07995972}\n",
            "INFO:tensorflow:Step 1800 per-step time 0.132s\n",
            "I1028 08:59:20.929595 139013598367744 model_lib_v2.py:705] Step 1800 per-step time 0.132s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.22616518,\n",
            " 'Loss/localization_loss': 0.02051521,\n",
            " 'Loss/regularization_loss': 0.14278492,\n",
            " 'Loss/total_loss': 0.3894653,\n",
            " 'learning_rate': 0.0799474}\n",
            "I1028 08:59:20.929933 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.22616518,\n",
            " 'Loss/localization_loss': 0.02051521,\n",
            " 'Loss/regularization_loss': 0.14278492,\n",
            " 'Loss/total_loss': 0.3894653,\n",
            " 'learning_rate': 0.0799474}\n",
            "INFO:tensorflow:Step 1900 per-step time 0.115s\n",
            "I1028 08:59:32.399732 139013598367744 model_lib_v2.py:705] Step 1900 per-step time 0.115s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.19527736,\n",
            " 'Loss/localization_loss': 0.021646095,\n",
            " 'Loss/regularization_loss': 0.1421208,\n",
            " 'Loss/total_loss': 0.35904425,\n",
            " 'learning_rate': 0.07993342}\n",
            "I1028 08:59:32.400110 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.19527736,\n",
            " 'Loss/localization_loss': 0.021646095,\n",
            " 'Loss/regularization_loss': 0.1421208,\n",
            " 'Loss/total_loss': 0.35904425,\n",
            " 'learning_rate': 0.07993342}\n",
            "INFO:tensorflow:Step 2000 per-step time 0.128s\n",
            "I1028 08:59:45.187060 139013598367744 model_lib_v2.py:705] Step 2000 per-step time 0.128s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.15905377,\n",
            " 'Loss/localization_loss': 0.037840433,\n",
            " 'Loss/regularization_loss': 0.14151354,\n",
            " 'Loss/total_loss': 0.33840775,\n",
            " 'learning_rate': 0.07991781}\n",
            "I1028 08:59:45.187499 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.15905377,\n",
            " 'Loss/localization_loss': 0.037840433,\n",
            " 'Loss/regularization_loss': 0.14151354,\n",
            " 'Loss/total_loss': 0.33840775,\n",
            " 'learning_rate': 0.07991781}\n",
            "INFO:tensorflow:Step 2100 per-step time 0.134s\n",
            "I1028 08:59:58.610438 139013598367744 model_lib_v2.py:705] Step 2100 per-step time 0.134s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1472134,\n",
            " 'Loss/localization_loss': 0.018300472,\n",
            " 'Loss/regularization_loss': 0.14089255,\n",
            " 'Loss/total_loss': 0.30640644,\n",
            " 'learning_rate': 0.07990056}\n",
            "I1028 08:59:58.610831 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.1472134,\n",
            " 'Loss/localization_loss': 0.018300472,\n",
            " 'Loss/regularization_loss': 0.14089255,\n",
            " 'Loss/total_loss': 0.30640644,\n",
            " 'learning_rate': 0.07990056}\n",
            "INFO:tensorflow:Step 2200 per-step time 0.119s\n",
            "I1028 09:00:10.505442 139013598367744 model_lib_v2.py:705] Step 2200 per-step time 0.119s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.16954052,\n",
            " 'Loss/localization_loss': 0.015721783,\n",
            " 'Loss/regularization_loss': 0.1403017,\n",
            " 'Loss/total_loss': 0.32556403,\n",
            " 'learning_rate': 0.07988167}\n",
            "I1028 09:00:10.505718 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.16954052,\n",
            " 'Loss/localization_loss': 0.015721783,\n",
            " 'Loss/regularization_loss': 0.1403017,\n",
            " 'Loss/total_loss': 0.32556403,\n",
            " 'learning_rate': 0.07988167}\n",
            "INFO:tensorflow:Step 2300 per-step time 0.127s\n",
            "I1028 09:00:23.255277 139013598367744 model_lib_v2.py:705] Step 2300 per-step time 0.127s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.15822773,\n",
            " 'Loss/localization_loss': 0.01958588,\n",
            " 'Loss/regularization_loss': 0.1396528,\n",
            " 'Loss/total_loss': 0.3174664,\n",
            " 'learning_rate': 0.07986114}\n",
            "I1028 09:00:23.255655 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.15822773,\n",
            " 'Loss/localization_loss': 0.01958588,\n",
            " 'Loss/regularization_loss': 0.1396528,\n",
            " 'Loss/total_loss': 0.3174664,\n",
            " 'learning_rate': 0.07986114}\n",
            "INFO:tensorflow:Step 2400 per-step time 0.113s\n",
            "I1028 09:00:34.556493 139013598367744 model_lib_v2.py:705] Step 2400 per-step time 0.113s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.18880804,\n",
            " 'Loss/localization_loss': 0.016477047,\n",
            " 'Loss/regularization_loss': 0.13908222,\n",
            " 'Loss/total_loss': 0.34436733,\n",
            " 'learning_rate': 0.07983897}\n",
            "I1028 09:00:34.556847 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.18880804,\n",
            " 'Loss/localization_loss': 0.016477047,\n",
            " 'Loss/regularization_loss': 0.13908222,\n",
            " 'Loss/total_loss': 0.34436733,\n",
            " 'learning_rate': 0.07983897}\n",
            "INFO:tensorflow:Step 2500 per-step time 0.128s\n",
            "I1028 09:00:47.406165 139013598367744 model_lib_v2.py:705] Step 2500 per-step time 0.128s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.15289924,\n",
            " 'Loss/localization_loss': 0.021198006,\n",
            " 'Loss/regularization_loss': 0.13858755,\n",
            " 'Loss/total_loss': 0.31268477,\n",
            " 'learning_rate': 0.079815164}\n",
            "I1028 09:00:47.406571 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.15289924,\n",
            " 'Loss/localization_loss': 0.021198006,\n",
            " 'Loss/regularization_loss': 0.13858755,\n",
            " 'Loss/total_loss': 0.31268477,\n",
            " 'learning_rate': 0.079815164}\n",
            "INFO:tensorflow:Step 2600 per-step time 0.111s\n",
            "I1028 09:00:58.451154 139013598367744 model_lib_v2.py:705] Step 2600 per-step time 0.111s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.13577628,\n",
            " 'Loss/localization_loss': 0.026223328,\n",
            " 'Loss/regularization_loss': 0.1379773,\n",
            " 'Loss/total_loss': 0.29997692,\n",
            " 'learning_rate': 0.07978972}\n",
            "I1028 09:00:58.451514 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.13577628,\n",
            " 'Loss/localization_loss': 0.026223328,\n",
            " 'Loss/regularization_loss': 0.1379773,\n",
            " 'Loss/total_loss': 0.29997692,\n",
            " 'learning_rate': 0.07978972}\n",
            "INFO:tensorflow:Step 2700 per-step time 0.133s\n",
            "I1028 09:01:11.766662 139013598367744 model_lib_v2.py:705] Step 2700 per-step time 0.133s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.14825581,\n",
            " 'Loss/localization_loss': 0.03499803,\n",
            " 'Loss/regularization_loss': 0.13735124,\n",
            " 'Loss/total_loss': 0.3206051,\n",
            " 'learning_rate': 0.07976264}\n",
            "I1028 09:01:11.766980 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.14825581,\n",
            " 'Loss/localization_loss': 0.03499803,\n",
            " 'Loss/regularization_loss': 0.13735124,\n",
            " 'Loss/total_loss': 0.3206051,\n",
            " 'learning_rate': 0.07976264}\n",
            "INFO:tensorflow:Step 2800 per-step time 0.108s\n",
            "I1028 09:01:22.567862 139013598367744 model_lib_v2.py:705] Step 2800 per-step time 0.108s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.14820954,\n",
            " 'Loss/localization_loss': 0.032572433,\n",
            " 'Loss/regularization_loss': 0.13671462,\n",
            " 'Loss/total_loss': 0.3174966,\n",
            " 'learning_rate': 0.07973392}\n",
            "I1028 09:01:22.568213 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.14820954,\n",
            " 'Loss/localization_loss': 0.032572433,\n",
            " 'Loss/regularization_loss': 0.13671462,\n",
            " 'Loss/total_loss': 0.3174966,\n",
            " 'learning_rate': 0.07973392}\n",
            "INFO:tensorflow:Step 2900 per-step time 0.134s\n",
            "I1028 09:01:35.965779 139013598367744 model_lib_v2.py:705] Step 2900 per-step time 0.134s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.13627535,\n",
            " 'Loss/localization_loss': 0.01953066,\n",
            " 'Loss/regularization_loss': 0.1361092,\n",
            " 'Loss/total_loss': 0.2919152,\n",
            " 'learning_rate': 0.07970358}\n",
            "I1028 09:01:35.966069 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.13627535,\n",
            " 'Loss/localization_loss': 0.01953066,\n",
            " 'Loss/regularization_loss': 0.1361092,\n",
            " 'Loss/total_loss': 0.2919152,\n",
            " 'learning_rate': 0.07970358}\n",
            "INFO:tensorflow:Step 3000 per-step time 0.112s\n",
            "I1028 09:01:47.226323 139013598367744 model_lib_v2.py:705] Step 3000 per-step time 0.112s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1493501,\n",
            " 'Loss/localization_loss': 0.015757754,\n",
            " 'Loss/regularization_loss': 0.1355271,\n",
            " 'Loss/total_loss': 0.30063498,\n",
            " 'learning_rate': 0.0796716}\n",
            "I1028 09:01:47.226786 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.1493501,\n",
            " 'Loss/localization_loss': 0.015757754,\n",
            " 'Loss/regularization_loss': 0.1355271,\n",
            " 'Loss/total_loss': 0.30063498,\n",
            " 'learning_rate': 0.0796716}\n",
            "INFO:tensorflow:Step 3100 per-step time 0.143s\n",
            "I1028 09:02:01.520357 139013598367744 model_lib_v2.py:705] Step 3100 per-step time 0.143s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.19982395,\n",
            " 'Loss/localization_loss': 0.011225923,\n",
            " 'Loss/regularization_loss': 0.13492411,\n",
            " 'Loss/total_loss': 0.34597397,\n",
            " 'learning_rate': 0.07963799}\n",
            "I1028 09:02:01.520673 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.19982395,\n",
            " 'Loss/localization_loss': 0.011225923,\n",
            " 'Loss/regularization_loss': 0.13492411,\n",
            " 'Loss/total_loss': 0.34597397,\n",
            " 'learning_rate': 0.07963799}\n",
            "INFO:tensorflow:Step 3200 per-step time 0.126s\n",
            "I1028 09:02:14.158016 139013598367744 model_lib_v2.py:705] Step 3200 per-step time 0.126s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.20027775,\n",
            " 'Loss/localization_loss': 0.024261199,\n",
            " 'Loss/regularization_loss': 0.13438378,\n",
            " 'Loss/total_loss': 0.35892272,\n",
            " 'learning_rate': 0.07960275}\n",
            "I1028 09:02:14.158408 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.20027775,\n",
            " 'Loss/localization_loss': 0.024261199,\n",
            " 'Loss/regularization_loss': 0.13438378,\n",
            " 'Loss/total_loss': 0.35892272,\n",
            " 'learning_rate': 0.07960275}\n",
            "INFO:tensorflow:Step 3300 per-step time 0.133s\n",
            "I1028 09:02:27.414550 139013598367744 model_lib_v2.py:705] Step 3300 per-step time 0.133s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.15490192,\n",
            " 'Loss/localization_loss': 0.029550904,\n",
            " 'Loss/regularization_loss': 0.13386078,\n",
            " 'Loss/total_loss': 0.3183136,\n",
            " 'learning_rate': 0.07956588}\n",
            "I1028 09:02:27.414809 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.15490192,\n",
            " 'Loss/localization_loss': 0.029550904,\n",
            " 'Loss/regularization_loss': 0.13386078,\n",
            " 'Loss/total_loss': 0.3183136,\n",
            " 'learning_rate': 0.07956588}\n",
            "INFO:tensorflow:Step 3400 per-step time 0.127s\n",
            "I1028 09:02:40.173088 139013598367744 model_lib_v2.py:705] Step 3400 per-step time 0.127s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.13911669,\n",
            " 'Loss/localization_loss': 0.019870061,\n",
            " 'Loss/regularization_loss': 0.1332944,\n",
            " 'Loss/total_loss': 0.29228115,\n",
            " 'learning_rate': 0.079527386}\n",
            "I1028 09:02:40.173443 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.13911669,\n",
            " 'Loss/localization_loss': 0.019870061,\n",
            " 'Loss/regularization_loss': 0.1332944,\n",
            " 'Loss/total_loss': 0.29228115,\n",
            " 'learning_rate': 0.079527386}\n",
            "INFO:tensorflow:Step 3500 per-step time 0.111s\n",
            "I1028 09:02:51.296299 139013598367744 model_lib_v2.py:705] Step 3500 per-step time 0.111s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.16395262,\n",
            " 'Loss/localization_loss': 0.012441111,\n",
            " 'Loss/regularization_loss': 0.13275075,\n",
            " 'Loss/total_loss': 0.3091445,\n",
            " 'learning_rate': 0.07948727}\n",
            "I1028 09:02:51.296630 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.16395262,\n",
            " 'Loss/localization_loss': 0.012441111,\n",
            " 'Loss/regularization_loss': 0.13275075,\n",
            " 'Loss/total_loss': 0.3091445,\n",
            " 'learning_rate': 0.07948727}\n",
            "INFO:tensorflow:Step 3600 per-step time 0.133s\n",
            "I1028 09:03:04.643957 139013598367744 model_lib_v2.py:705] Step 3600 per-step time 0.133s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.19142535,\n",
            " 'Loss/localization_loss': 0.023188455,\n",
            " 'Loss/regularization_loss': 0.13216028,\n",
            " 'Loss/total_loss': 0.3467741,\n",
            " 'learning_rate': 0.079445526}\n",
            "I1028 09:03:04.644461 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.19142535,\n",
            " 'Loss/localization_loss': 0.023188455,\n",
            " 'Loss/regularization_loss': 0.13216028,\n",
            " 'Loss/total_loss': 0.3467741,\n",
            " 'learning_rate': 0.079445526}\n",
            "INFO:tensorflow:Step 3700 per-step time 0.108s\n",
            "I1028 09:03:15.402811 139013598367744 model_lib_v2.py:705] Step 3700 per-step time 0.108s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.26439026,\n",
            " 'Loss/localization_loss': 0.03150947,\n",
            " 'Loss/regularization_loss': 0.13168514,\n",
            " 'Loss/total_loss': 0.42758486,\n",
            " 'learning_rate': 0.07940216}\n",
            "I1028 09:03:15.403191 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.26439026,\n",
            " 'Loss/localization_loss': 0.03150947,\n",
            " 'Loss/regularization_loss': 0.13168514,\n",
            " 'Loss/total_loss': 0.42758486,\n",
            " 'learning_rate': 0.07940216}\n",
            "INFO:tensorflow:Step 3800 per-step time 0.136s\n",
            "I1028 09:03:29.001556 139013598367744 model_lib_v2.py:705] Step 3800 per-step time 0.136s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.18193409,\n",
            " 'Loss/localization_loss': 0.016462432,\n",
            " 'Loss/regularization_loss': 0.13113528,\n",
            " 'Loss/total_loss': 0.3295318,\n",
            " 'learning_rate': 0.079357184}\n",
            "I1028 09:03:29.001843 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.18193409,\n",
            " 'Loss/localization_loss': 0.016462432,\n",
            " 'Loss/regularization_loss': 0.13113528,\n",
            " 'Loss/total_loss': 0.3295318,\n",
            " 'learning_rate': 0.079357184}\n",
            "INFO:tensorflow:Step 3900 per-step time 0.111s\n",
            "I1028 09:03:40.141032 139013598367744 model_lib_v2.py:705] Step 3900 per-step time 0.111s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.13010316,\n",
            " 'Loss/localization_loss': 0.0140043795,\n",
            " 'Loss/regularization_loss': 0.1306027,\n",
            " 'Loss/total_loss': 0.27471024,\n",
            " 'learning_rate': 0.07931058}\n",
            "I1028 09:03:40.141388 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.13010316,\n",
            " 'Loss/localization_loss': 0.0140043795,\n",
            " 'Loss/regularization_loss': 0.1306027,\n",
            " 'Loss/total_loss': 0.27471024,\n",
            " 'learning_rate': 0.07931058}\n",
            "INFO:tensorflow:Step 4000 per-step time 0.130s\n",
            "I1028 09:03:53.033122 139013598367744 model_lib_v2.py:705] Step 4000 per-step time 0.130s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1252121,\n",
            " 'Loss/localization_loss': 0.015621167,\n",
            " 'Loss/regularization_loss': 0.13016357,\n",
            " 'Loss/total_loss': 0.27099684,\n",
            " 'learning_rate': 0.07926236}\n",
            "I1028 09:03:53.033442 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.1252121,\n",
            " 'Loss/localization_loss': 0.015621167,\n",
            " 'Loss/regularization_loss': 0.13016357,\n",
            " 'Loss/total_loss': 0.27099684,\n",
            " 'learning_rate': 0.07926236}\n",
            "INFO:tensorflow:Step 4100 per-step time 0.130s\n",
            "I1028 09:04:06.078692 139013598367744 model_lib_v2.py:705] Step 4100 per-step time 0.130s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.12876432,\n",
            " 'Loss/localization_loss': 0.01788802,\n",
            " 'Loss/regularization_loss': 0.12963676,\n",
            " 'Loss/total_loss': 0.2762891,\n",
            " 'learning_rate': 0.07921253}\n",
            "I1028 09:04:06.078992 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.12876432,\n",
            " 'Loss/localization_loss': 0.01788802,\n",
            " 'Loss/regularization_loss': 0.12963676,\n",
            " 'Loss/total_loss': 0.2762891,\n",
            " 'learning_rate': 0.07921253}\n",
            "INFO:tensorflow:Step 4200 per-step time 0.124s\n",
            "I1028 09:04:18.430049 139013598367744 model_lib_v2.py:705] Step 4200 per-step time 0.124s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.25256646,\n",
            " 'Loss/localization_loss': 0.034476247,\n",
            " 'Loss/regularization_loss': 0.12922698,\n",
            " 'Loss/total_loss': 0.4162697,\n",
            " 'learning_rate': 0.07916109}\n",
            "I1028 09:04:18.430331 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.25256646,\n",
            " 'Loss/localization_loss': 0.034476247,\n",
            " 'Loss/regularization_loss': 0.12922698,\n",
            " 'Loss/total_loss': 0.4162697,\n",
            " 'learning_rate': 0.07916109}\n",
            "INFO:tensorflow:Step 4300 per-step time 0.123s\n",
            "I1028 09:04:30.775292 139013598367744 model_lib_v2.py:705] Step 4300 per-step time 0.123s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.115197025,\n",
            " 'Loss/localization_loss': 0.017213538,\n",
            " 'Loss/regularization_loss': 0.12870039,\n",
            " 'Loss/total_loss': 0.26111096,\n",
            " 'learning_rate': 0.07910804}\n",
            "I1028 09:04:30.775695 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.115197025,\n",
            " 'Loss/localization_loss': 0.017213538,\n",
            " 'Loss/regularization_loss': 0.12870039,\n",
            " 'Loss/total_loss': 0.26111096,\n",
            " 'learning_rate': 0.07910804}\n",
            "INFO:tensorflow:Step 4400 per-step time 0.118s\n",
            "I1028 09:04:42.586455 139013598367744 model_lib_v2.py:705] Step 4400 per-step time 0.118s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.14287086,\n",
            " 'Loss/localization_loss': 0.010482089,\n",
            " 'Loss/regularization_loss': 0.12822464,\n",
            " 'Loss/total_loss': 0.2815776,\n",
            " 'learning_rate': 0.07905338}\n",
            "I1028 09:04:42.586735 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.14287086,\n",
            " 'Loss/localization_loss': 0.010482089,\n",
            " 'Loss/regularization_loss': 0.12822464,\n",
            " 'Loss/total_loss': 0.2815776,\n",
            " 'learning_rate': 0.07905338}\n",
            "INFO:tensorflow:Step 4500 per-step time 0.129s\n",
            "I1028 09:04:55.477108 139013598367744 model_lib_v2.py:705] Step 4500 per-step time 0.129s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2695628,\n",
            " 'Loss/localization_loss': 0.0197866,\n",
            " 'Loss/regularization_loss': 0.12782604,\n",
            " 'Loss/total_loss': 0.41717544,\n",
            " 'learning_rate': 0.07899711}\n",
            "I1028 09:04:55.477497 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.2695628,\n",
            " 'Loss/localization_loss': 0.0197866,\n",
            " 'Loss/regularization_loss': 0.12782604,\n",
            " 'Loss/total_loss': 0.41717544,\n",
            " 'learning_rate': 0.07899711}\n",
            "INFO:tensorflow:Step 4600 per-step time 0.114s\n",
            "I1028 09:05:06.849053 139013598367744 model_lib_v2.py:705] Step 4600 per-step time 0.114s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.13282336,\n",
            " 'Loss/localization_loss': 0.013153482,\n",
            " 'Loss/regularization_loss': 0.12735783,\n",
            " 'Loss/total_loss': 0.27333468,\n",
            " 'learning_rate': 0.078939244}\n",
            "I1028 09:05:06.849354 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.13282336,\n",
            " 'Loss/localization_loss': 0.013153482,\n",
            " 'Loss/regularization_loss': 0.12735783,\n",
            " 'Loss/total_loss': 0.27333468,\n",
            " 'learning_rate': 0.078939244}\n",
            "INFO:tensorflow:Step 4700 per-step time 0.134s\n",
            "I1028 09:05:20.224221 139013598367744 model_lib_v2.py:705] Step 4700 per-step time 0.134s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.15908617,\n",
            " 'Loss/localization_loss': 0.047892578,\n",
            " 'Loss/regularization_loss': 0.1268879,\n",
            " 'Loss/total_loss': 0.33386666,\n",
            " 'learning_rate': 0.07887978}\n",
            "I1028 09:05:20.224643 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.15908617,\n",
            " 'Loss/localization_loss': 0.047892578,\n",
            " 'Loss/regularization_loss': 0.1268879,\n",
            " 'Loss/total_loss': 0.33386666,\n",
            " 'learning_rate': 0.07887978}\n",
            "INFO:tensorflow:Step 4800 per-step time 0.110s\n",
            "I1028 09:05:31.239743 139013598367744 model_lib_v2.py:705] Step 4800 per-step time 0.110s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1535021,\n",
            " 'Loss/localization_loss': 0.019640902,\n",
            " 'Loss/regularization_loss': 0.12651725,\n",
            " 'Loss/total_loss': 0.29966027,\n",
            " 'learning_rate': 0.07881871}\n",
            "I1028 09:05:31.240108 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.1535021,\n",
            " 'Loss/localization_loss': 0.019640902,\n",
            " 'Loss/regularization_loss': 0.12651725,\n",
            " 'Loss/total_loss': 0.29966027,\n",
            " 'learning_rate': 0.07881871}\n",
            "INFO:tensorflow:Step 4900 per-step time 0.134s\n",
            "I1028 09:05:44.609769 139013598367744 model_lib_v2.py:705] Step 4900 per-step time 0.134s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.12602545,\n",
            " 'Loss/localization_loss': 0.015313011,\n",
            " 'Loss/regularization_loss': 0.12614147,\n",
            " 'Loss/total_loss': 0.26747996,\n",
            " 'learning_rate': 0.07875605}\n",
            "I1028 09:05:44.610113 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.12602545,\n",
            " 'Loss/localization_loss': 0.015313011,\n",
            " 'Loss/regularization_loss': 0.12614147,\n",
            " 'Loss/total_loss': 0.26747996,\n",
            " 'learning_rate': 0.07875605}\n",
            "INFO:tensorflow:Step 5000 per-step time 0.125s\n",
            "I1028 09:05:57.149991 139013598367744 model_lib_v2.py:705] Step 5000 per-step time 0.125s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.20269507,\n",
            " 'Loss/localization_loss': 0.024502145,\n",
            " 'Loss/regularization_loss': 0.12569864,\n",
            " 'Loss/total_loss': 0.35289586,\n",
            " 'learning_rate': 0.078691795}\n",
            "I1028 09:05:57.150348 139013598367744 model_lib_v2.py:708] {'Loss/classification_loss': 0.20269507,\n",
            " 'Loss/localization_loss': 0.024502145,\n",
            " 'Loss/regularization_loss': 0.12569864,\n",
            " 'Loss/total_loss': 0.35289586,\n",
            " 'learning_rate': 0.078691795}\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_YRZu7npfDH"
      },
      "source": [
        "# 7. Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "80L7-fdPpfDH"
      },
      "outputs": [],
      "source": [
        "# define the evaluate function\n",
        "command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqTV2jGBpfDH",
        "outputId": "8af90b59-a57f-468e-819c-15d79b7f2d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-28 09:06:05.713285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W1028 09:06:09.178299 132085040590848 model_lib_v2.py:1089] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\n",
            "I1028 09:06:09.178540 132085040590848 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I1028 09:06:09.178631 132085040590848 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I1028 09:06:09.178720 132085040590848 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W1028 09:06:09.178837 132085040590848 model_lib_v2.py:1106] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "2023-10-28 09:06:10.190238: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Reading unweighted datasets: ['Tensorflow/workspace/annotations/test.record']\n",
            "I1028 09:06:10.243116 132085040590848 dataset_builder.py:162] Reading unweighted datasets: ['Tensorflow/workspace/annotations/test.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['Tensorflow/workspace/annotations/test.record']\n",
            "I1028 09:06:10.243365 132085040590848 dataset_builder.py:79] Reading record datasets for input file: ['Tensorflow/workspace/annotations/test.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I1028 09:06:10.243476 132085040590848 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1028 09:06:10.243551 132085040590848 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W1028 09:06:10.248052 132085040590848 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1028 09:06:10.268855 132085040590848 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W1028 09:06:14.040322 132085040590848 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W1028 09:06:14.961517 132085040590848 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Waiting for new checkpoint at Tensorflow/workspace/models/my_ssd_mobnet\n",
            "I1028 09:06:17.433919 132085040590848 checkpoint_utils.py:168] Waiting for new checkpoint at Tensorflow/workspace/models/my_ssd_mobnet\n",
            "INFO:tensorflow:Found new checkpoint at Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6\n",
            "I1028 09:06:17.434683 132085040590848 checkpoint_utils.py:177] Found new checkpoint at Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn(\n",
            "I1028 09:06:26.357722 132085040590848 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "I1028 09:06:39.503271 132085040590848 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W1028 09:06:47.841580 132085040590848 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Finished eval step 0\n",
            "I1028 09:06:47.888758 132085040590848 model_lib_v2.py:966] Finished eval step 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:460: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W1028 09:06:48.467956 132085040590848 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:460: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "INFO:tensorflow:Performing evaluation on 94 images.\n",
            "I1028 09:06:58.712294 132085040590848 coco_evaluation.py:293] Performing evaluation on 94 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I1028 09:06:58.714375 132085040590848 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I1028 09:06:58.721320 132085040590848 coco_tools.py:138] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.608\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.599\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.488\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.818\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.822\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.822\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.822\n",
            "INFO:tensorflow:Eval metrics at step 5000\n",
            "I1028 09:06:59.060707 132085040590848 model_lib_v2.py:1015] Eval metrics at step 5000\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP: 0.488171\n",
            "I1028 09:06:59.072622 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP: 0.488171\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP@.50IOU: 0.607748\n",
            "I1028 09:06:59.074342 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP@.50IOU: 0.607748\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP@.75IOU: 0.599211\n",
            "I1028 09:06:59.075960 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP@.75IOU: 0.599211\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (small): -1.000000\n",
            "I1028 09:06:59.077490 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP (small): -1.000000\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (medium): -1.000000\n",
            "I1028 09:06:59.078791 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP (medium): -1.000000\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (large): 0.488171\n",
            "I1028 09:06:59.079995 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP (large): 0.488171\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@1: 0.817778\n",
            "I1028 09:06:59.081565 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@1: 0.817778\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@10: 0.822361\n",
            "I1028 09:06:59.083104 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@10: 0.822361\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100: 0.822361\n",
            "I1028 09:06:59.084315 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@100: 0.822361\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (small): -1.000000\n",
            "I1028 09:06:59.085519 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@100 (small): -1.000000\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (medium): -1.000000\n",
            "I1028 09:06:59.086712 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@100 (medium): -1.000000\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (large): 0.822361\n",
            "I1028 09:06:59.087920 132085040590848 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@100 (large): 0.822361\n",
            "INFO:tensorflow:\t+ Loss/localization_loss: 0.056130\n",
            "I1028 09:06:59.088941 132085040590848 model_lib_v2.py:1018] \t+ Loss/localization_loss: 0.056130\n",
            "INFO:tensorflow:\t+ Loss/classification_loss: 0.299569\n",
            "I1028 09:06:59.089975 132085040590848 model_lib_v2.py:1018] \t+ Loss/classification_loss: 0.299569\n",
            "INFO:tensorflow:\t+ Loss/regularization_loss: 0.125694\n",
            "I1028 09:06:59.091121 132085040590848 model_lib_v2.py:1018] \t+ Loss/regularization_loss: 0.125694\n",
            "INFO:tensorflow:\t+ Loss/total_loss: 0.481393\n",
            "I1028 09:06:59.092205 132085040590848 model_lib_v2.py:1018] \t+ Loss/total_loss: 0.481393\n",
            "INFO:tensorflow:Waiting for new checkpoint at Tensorflow/workspace/models/my_ssd_mobnet\n",
            "I1028 09:11:17.509897 132085040590848 checkpoint_utils.py:168] Waiting for new checkpoint at Tensorflow/workspace/models/my_ssd_mobnet\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orvRk02UpfDI"
      },
      "source": [
        "# 8. Load Train Model From Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TYk4_oIpfDI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDnQg-cYpfDI"
      },
      "outputs": [],
      "source": [
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
        "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-6')).expect_partial()\n",
        "\n",
        "@tf.function\n",
        "def detect_fn(image):\n",
        "    image, shapes = detection_model.preprocess(image)\n",
        "    prediction_dict = detection_model.predict(image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EmsmbBZpfDI"
      },
      "source": [
        "# 9. Detect from an Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_MKiuZ4pfDI"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBDbIhNapfDI"
      },
      "outputs": [],
      "source": [
        "# create category index from label map for visualisation\n",
        "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx3crOhOzITB"
      },
      "outputs": [],
      "source": [
        "# define the path of the image to be passed into model\n",
        "IMAGE_PATH = os.path.join(paths['IMAGE_PATH'], 'train', 'BrokenBig-10-_bmp.rf.889d5780aa1c7e6ed1c837a23f7bd76a.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djJVE1r0f71y"
      },
      "outputs": [],
      "source": [
        "# install arial for label purposes\n",
        "!wget https://freefontsdownload.net/download/160187/arial.zip\n",
        "!unzip arial.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tpzn1SMry1yK"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "# read image and convert to tensor\n",
        "img = cv2.imread(IMAGE_PATH)\n",
        "image_np = np.array(img)\n",
        "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "\n",
        "# detect within the image\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "# get the number of detections and the detections array\n",
        "num_detections = int(detections.pop('num_detections'))\n",
        "detections = {key: value[0, :num_detections].numpy()\n",
        "              for key, value in detections.items()}\n",
        "detections['num_detections'] = num_detections\n",
        "\n",
        "# detection_classes should be ints\n",
        "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "# define label offset\n",
        "label_id_offset = 1\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "# draw bounding box on the image passed in based on the configurations\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "            image_np_with_detections,\n",
        "            detections['detection_boxes'],\n",
        "            detections['detection_classes']+label_id_offset,\n",
        "            detections['detection_scores'],\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            max_boxes_to_draw=5,\n",
        "            agnostic_mode=False,\n",
        "            line_thickness=8,\n",
        "            min_score_thresh=0.4)\n",
        "\n",
        "# show the image\n",
        "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# print out the detected class and confidence level\n",
        "print(f\"Detected class: {detections['detection_classes'][0]}\\nConfidence level: {detections['detection_scores'][0] * 100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNAaYAo0WVL"
      },
      "source": [
        "# Real Time Detections from your Webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc-GpMbcajWa"
      },
      "outputs": [],
      "source": [
        "!pip uninstall opencv-python-headless -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the dimensions\n",
        "cap = cv2.VideoCapture(0)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "while cap.isOpened():\n",
        "    # read frame from webcam and convert to tensor\n",
        "    ret, frame = cap.read()\n",
        "    image_np = np.array(frame)\n",
        "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "\n",
        "    # detect within the frame\n",
        "    detections = detect_fn(input_tensor)\n",
        "\n",
        "    # get the number of detections and the detections array\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                  for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "\n",
        "    # detection_classes should be ints.\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "    # define label offset\n",
        "    label_id_offset = 1\n",
        "    image_np_with_detections = image_np.copy()\n",
        "\n",
        "    # draw bounding box on the image passed in based on the configurations\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "                image_np_with_detections,\n",
        "                detections['detection_boxes'],\n",
        "                detections['detection_classes']+label_id_offset,\n",
        "                detections['detection_scores'],\n",
        "                category_index,\n",
        "                use_normalized_coordinates=True,\n",
        "                max_boxes_to_draw=5,\n",
        "                agnostic_mode=False)\n",
        "\n",
        "    # show the result live\n",
        "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
        "\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym6h-WeA1vEz"
      },
      "source": [
        "## Generate Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnmqdtXF1uyK"
      },
      "outputs": [],
      "source": [
        "types = ('*.png', '*.jpg') # the tuple of file types\n",
        "files_grabbed = []\n",
        "for file in types:\n",
        " files_grabbed.extend(glob.glob(os.path.join(paths['IMAGE_PATH'], 'test', file)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdRRN6pr2Xii"
      },
      "outputs": [],
      "source": [
        "# define test images paths constant\n",
        "TEST_IMAGE_PATHS = files_grabbed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhuI04Oy7BvF"
      },
      "outputs": [],
      "source": [
        "# labels array\n",
        "labels = [\"0\", \"1\", \"2\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm_lT8H_75Ef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "test_pred = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foXkyaXOAe-z"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Suppress Matplotlib warnings\n",
        "\n",
        "def load_image_into_numpy_array(image_path):\n",
        "  return np.array(Image.open(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6i9I9Y9AoIN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# loop through all the test images\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "\n",
        "  # load the image using numpy\n",
        "  image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "  # rename test image path and get the height and width of the image\n",
        "  image_path = image_path.replace(\"Tensorflow/workspace/images/test/\", \"\")\n",
        "  height, width, color = image_np.shape\n",
        "\n",
        "  # convert images to tensor\n",
        "  input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "\n",
        "  # detect within the image\n",
        "  detections = detect_fn(input_tensor)\n",
        "\n",
        "  # get the number of detections and the detections array\n",
        "  num_detections = int(detections.pop('num_detections'))\n",
        "  detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
        "  detections['num_detections'] = num_detections\n",
        "  detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "  # create a new row of detection information to be written out to a CSV file\n",
        "  ll = []\n",
        "  ll.append(image_path)\n",
        "  ll.append(width)\n",
        "  ll.append(height)\n",
        "  ll.append(detections['detection_boxes'][0].tolist())\n",
        "  ll.append(labels[int(detections['detection_classes'][0])])\n",
        "  ll.append(detections['detection_scores'][0])\n",
        "\n",
        "  pre_df = pd.DataFrame([{'filename':ll[0],\n",
        "    'width':ll[1],\n",
        "    'height':ll[2],\n",
        "    'class':ll[4],\n",
        "    'xmin':ll[3][1] * width,\n",
        "    'ymin':ll[3][0] * height,\n",
        "    'xmax':ll[3][3] * width,\n",
        "    'ymax':ll[3][2] * height,\n",
        "    'score':ll[5]}])\n",
        "\n",
        "  # append to the test prediction array\n",
        "  test_pred = pd.concat([test_pred, pre_df], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0VMyDMP8Z0T"
      },
      "outputs": [],
      "source": [
        "# write out prediction results to CSV file for confusion matrix generation\n",
        "test_pred.to_csv(os.path.join(paths['ANNOTATION_PATH'], 'test_pred.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL0B02HoEMzy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Mon Sep 14 21:42:19 2020\n",
        "@author: sujith\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# define paths\n",
        "test = pd.read_csv(os.path.join(paths['ANNOTATION_PATH'], 'test.csv'))\n",
        "pred = pd.read_csv(os.path.join(paths['ANNOTATION_PATH'], 'test_pred.csv'))\n",
        "test['filename'] = test['filename'].apply(lambda x : x.split('/')[-1])\n",
        "pred['filename'] = pred['filename'].apply(lambda x : x.split('/')[-1])\n",
        "output_path = os.path.join(paths['ANNOTATION_PATH'], 'confusion_matrix.csv')\n",
        "categories = [0, 1, 2]\n",
        "\n",
        "def process_detections(test, categories):\n",
        "    confusion_matrix = np.zeros(shape=(len(categories) + 1, len(categories) + 1))\n",
        "    file_unique = test['filename'].unique()\n",
        "    for file in file_unique:\n",
        "        # get the actual row of detection results of the test image\n",
        "        test_df = test[test['filename']==file]\n",
        "        test_df.reset_index(inplace = True, drop = True)\n",
        "\n",
        "        # get the predicted row of detection results of the test image\n",
        "        pred_df = pred[pred['filename']==file]\n",
        "        pred_df.reset_index(inplace = True, drop = True)\n",
        "\n",
        "        # get the actual class of the test image\n",
        "        test_class = test_df['class']\n",
        "        # get the predicted class of the test image\n",
        "        pred_class = pred_df['class']\n",
        "\n",
        "        # increment in confusion matrix\n",
        "        confusion_matrix[int(pred_class)][int(test_class)] += 1\n",
        "\n",
        "    return confusion_matrix\n",
        "\n",
        "\"\"\"\n",
        "Display the confusion matrix output and other important metrics (precision and recall) by class.\n",
        "\"\"\"\n",
        "def display(confusion_matrix, test, categories, output_path):\n",
        "    results = []\n",
        "    class_uniq = test['class'].unique()\n",
        "    for label in class_uniq:\n",
        "        class_id = int(float(categories[label]))\n",
        "        name = label\n",
        "\n",
        "        total_target = np.sum(confusion_matrix[class_id,:])\n",
        "        total_predicted = np.sum(confusion_matrix[:,class_id])\n",
        "\n",
        "        precision = float(confusion_matrix[class_id, class_id] / total_predicted)\n",
        "        recall = float(confusion_matrix[class_id, class_id] / total_target)\n",
        "\n",
        "        results.append({'category' : name, 'precision': precision, 'recall' : recall})\n",
        "    print(confusion_matrix)\n",
        "    print(precision)\n",
        "    print(recall)\n",
        "    df = pd.DataFrame(results)\n",
        "    print(df)\n",
        "    df.to_csv(output_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    confusion_matrix = process_detections(test,categories)\n",
        "    display(confusion_matrix, test, categories, output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzlM4jt0pfDJ"
      },
      "source": [
        "# 10. Freezing the Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4olHB2npfDJ"
      },
      "outputs": [],
      "source": [
        "# define the freeze script path\n",
        "FREEZE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'exporter_main_v2.py ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AjO93QDpfDJ"
      },
      "outputs": [],
      "source": [
        "# define freeze function for model weights\n",
        "command = \"python {} --input_type=image_tensor --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(FREEZE_SCRIPT , files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['OUTPUT_PATH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Sw1ULgHpfDJ"
      },
      "outputs": [],
      "source": [
        "# run the freeze function\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zH2nfvuWLtr"
      },
      "outputs": [],
      "source": [
        "# zip the savedmodel file\n",
        "command = \"zip -r {} {}\".format(os.path.join(paths['OUTPUT_PATH'], 'saved_model.zip'), os.path.join(paths['OUTPUT_PATH'], 'saved_model'))\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTPmdqaXpfDK"
      },
      "source": [
        "# 11. Conversion to TFJS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ6UzY_fpfDK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oxbVynHpfDK"
      },
      "outputs": [],
      "source": [
        "# define function to export model to tfjs format\n",
        "command = \"tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes,detection_classes,detection_features,detection_multiclass_scores,detection_scores,num_detections,raw_detection_boxes,raw_detection_scores' --output_format=tfjs_graph_model --signature_name=serving_default {} {}\".format(os.path.join(paths['OUTPUT_PATH'], 'saved_model'), paths['TFJS_PATH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7rfT4-hpfDK"
      },
      "outputs": [],
      "source": [
        "# export model to tfjs format\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtUw73FHpfDK"
      },
      "source": [
        "# 12. Conversion to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XviMtewLpfDK"
      },
      "outputs": [],
      "source": [
        "# define path to script for exporting to frozen tflite \n",
        "TFLITE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'export_tflite_graph_tf2.py ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us86cjC4pfDL"
      },
      "outputs": [],
      "source": [
        "# define function to export model to frozen tflite format\n",
        "command = \"python {} --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(TFLITE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['TFLITE_PATH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-xWpHN8pfDL"
      },
      "outputs": [],
      "source": [
        "# export model to frozen tflite format\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJfYMbN6pfDL"
      },
      "outputs": [],
      "source": [
        "# define paths\n",
        "FROZEN_TFLITE_PATH = os.path.join(paths['TFLITE_PATH'], 'saved_model')\n",
        "TFLITE_MODEL = os.path.join(paths['TFLITE_PATH'], 'saved_model', 'detect.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vozaU-R7ajWb"
      },
      "outputs": [],
      "source": [
        "# define function to convert frozen tflite file to tflite file\n",
        "command = \"tflite_convert \\\n",
        "--saved_model_dir={} \\\n",
        "--output_file={} \\\n",
        "--input_shapes=1,300,300,3 \\\n",
        "--input_arrays=normalized_input_image_tensor \\\n",
        "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
        "--inference_type=FLOAT \\\n",
        "--allow_custom_ops\".format(FROZEN_TFLITE_PATH, TFLITE_MODEL, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbd7gqHMpfDL"
      },
      "outputs": [],
      "source": [
        "# convert frozen tflite file to tflite file\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NQqZRdA21Uc"
      },
      "source": [
        "# 13. Zip and Export Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQCnWwdjZjyH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTVTGCQp2ZJJ"
      },
      "outputs": [],
      "source": [
        "# zip model\n",
        "!tar -czf models.tar.gz {paths['CHECKPOINT_PATH']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whShhB0x3PYJ"
      },
      "outputs": [],
      "source": [
        "# mount on google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tfod",
      "language": "python",
      "name": "tfod"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
